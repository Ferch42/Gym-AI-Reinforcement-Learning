{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout \n",
    "from keras.optimizers import Adam\n",
    "#from keras.constraints import MaxNorm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Board auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    #Attach a lot of summaries to a Tensor (for TensorBoard visualization).\n",
    "    for i in range(len(var)):\n",
    "        with tf.name_scope(\"Layer_\"+str(i)):\n",
    "            with tf.name_scope('summaries'):\n",
    "                mean = tf.reduce_mean(var[i])\n",
    "                tf.summary.scalar('mean', mean)\n",
    "                with tf.name_scope('stddev'):\n",
    "                    stddev = tf.sqrt(tf.reduce_mean(tf.square(var[i] - mean)))\n",
    "                tf.summary.scalar('stddev', stddev)\n",
    "                tf.summary.scalar('max', tf.reduce_max(var[i]))\n",
    "                tf.summary.scalar('min', tf.reduce_min(var[i]))\n",
    "                tf.summary.histogram('histogram', var[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries2(var):\n",
    "    #Attach a lot of summaries to a Tensor (for TensorBoard visualization).\n",
    "    \n",
    "    with tf.name_scope('summaries'):\n",
    "        tf.summary.scalar('value', var)\n",
    "        tf.summary.histogram('histogram', var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the use of GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/gpu:0']\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "sess= K.get_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Humanoid-v2')\n",
    "So = env.reset()\n",
    "A = env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting learning hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Policy\n",
    "policy_alpha = 0.01\n",
    "policy_lambda =  0.9\n",
    "#Value\n",
    "value_alpha = 0.01\n",
    "value_lambda = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model for policy mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Mean network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_1 (Dropout)          (None, 376)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               150800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 17)                6817      \n",
      "=================================================================\n",
      "Total params: 638,817\n",
      "Trainable params: 638,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "policy_model_mean = Sequential()\n",
    "policy_model_mean.add(Dropout(0.1,input_shape=So.shape))\n",
    "policy_model_mean.add(Dense(400 ,kernel_initializer='random_uniform', activation = 'relu' ))\n",
    "policy_model_mean.add(Dropout(0.5))\n",
    "policy_model_mean.add(Dense(400 ,kernel_initializer='random_uniform', activation = 'relu' ))\n",
    "policy_model_mean.add(Dropout(0.5))\n",
    "policy_model_mean.add(Dense(400,kernel_initializer='random_uniform', activation = 'relu'))\n",
    "policy_model_mean.add(Dropout(0.5))\n",
    "policy_model_mean.add(Dense(400 ,kernel_initializer='random_uniform', activation = 'relu' ))\n",
    "policy_model_mean.add(Dropout(0.5))\n",
    "policy_model_mean.add(Dense( A.shape[0],kernel_initializer='random_uniform', activation='linear' ))\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "policy_model_mean.compile(loss=\"mse\", optimizer= adam)\n",
    "print(\"Policy Mean network\")\n",
    "print(policy_model_mean.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model for policy std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy std network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_6 (Dropout)          (None, 376)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 400)               150800    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 17)                6817      \n",
      "=================================================================\n",
      "Total params: 638,817\n",
      "Trainable params: 638,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "policy_model_std = Sequential()\n",
    "policy_model_std.add(Dropout(0.1,input_shape=So.shape))\n",
    "policy_model_std.add(Dense(400 ,kernel_initializer='random_uniform', activation = 'relu' ))\n",
    "policy_model_std.add(Dropout(0.5))\n",
    "policy_model_std.add(Dense(400 ,kernel_initializer='random_uniform', activation = 'relu' ))\n",
    "policy_model_std.add(Dropout(0.5))\n",
    "policy_model_std.add(Dense(400,kernel_initializer='random_uniform', activation = 'relu'))\n",
    "policy_model_std.add(Dropout(0.5))\n",
    "policy_model_std.add(Dense(400,kernel_initializer='random_uniform', activation = 'relu'))\n",
    "policy_model_std.add(Dropout(0.5))\n",
    "policy_model_std.add(Dense( A.shape[0],kernel_initializer='random_uniform', activation='linear' ))\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "policy_model_std.compile(loss=\"mse\", optimizer= adam)\n",
    "print(\"Policy std network\")\n",
    "print(policy_model_std.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating model for values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_11 (Dropout)         (None, 376)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 400)               150800    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 632,401\n",
      "Trainable params: 632,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "value_model = Sequential()\n",
    "value_model.add(Dropout(0.1,input_shape=So.shape))\n",
    "value_model.add(Dense(400, kernel_initializer='random_uniform',activation = 'relu' ))\n",
    "value_model.add(Dropout(0.5))\n",
    "value_model.add(Dense(400, kernel_initializer='random_uniform',activation = 'relu'))\n",
    "value_model.add(Dropout(0.5))\n",
    "value_model.add(Dense(400, kernel_initializer='random_uniform',activation = 'relu'))\n",
    "value_model.add(Dropout(0.5))\n",
    "value_model.add(Dense(400, kernel_initializer='random_uniform',activation = 'relu'))\n",
    "value_model.add(Dropout(0.5))\n",
    "value_model.add(Dense(1,kernel_initializer='random_uniform', activation= 'linear'))\n",
    "\n",
    "print(\"Value Model\")\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "value_model.compile(loss=\"mse\", optimizer= adam)\n",
    "print(value_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eligibility Traces for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "with tf.name_scope(\"eligibility_traces\"):\n",
    "    with tf.name_scope(\"MEAN\"):\n",
    "        policy_eligibility_traces_mean= [tf.Variable(np.zeros(shape = sess.run(tf.shape(tensor))), dtype= tf.float32) for tensor in policy_model_mean.trainable_weights]\n",
    "        variable_summaries(policy_eligibility_traces_mean)\n",
    "    with tf.name_scope(\"STD\"):\n",
    "        policy_eligibility_traces_std= [tf.Variable(np.zeros(shape = sess.run(tf.shape(tensor))), dtype= tf.float32) for tensor in policy_model_std.trainable_weights]\n",
    "        variable_summaries(policy_eligibility_traces_std)\n",
    "    with tf.name_scope(\"VALUE\"):\n",
    "        value_eligibility_traces= [tf.Variable(np.zeros(shape = sess.run(tf.shape(tensor))), dtype= tf.float32) for tensor in value_model.trainable_weights]\n",
    "        variable_summaries(value_eligibility_traces)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining other hyperparameters concerning the reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_reward = 5 #pickle.load(open('average_reward.ferch','rb'))\n",
    "cumulative_reward=0\n",
    "etaa = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(False):\n",
    "    policy_model_mean = load_model('policy_mean.h5')\n",
    "    policy_model_std = load_model('policy_std.h5')\n",
    "    value_model = load_model('value.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions for weights update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_weights(model, weights):  \n",
    "    if(model=='value'):\n",
    "        i=0\n",
    "        for layer in value_model.layers:\n",
    "            if 'dropout' not in layer.name:\n",
    "                layer.set_weights([weights[i],weights[i+1]])\n",
    "                i+=2\n",
    "    elif(model=='policy_mean'):\n",
    "        i=0\n",
    "        for layerz in policy_model_mean.layers:\n",
    "            if 'dropout' not in layerz.name:\n",
    "                layerz.set_weights([weights[i],weights[i+1]])\n",
    "                i+=2\n",
    "                \n",
    "    elif(model=='policy_std'):\n",
    "        i=0\n",
    "        for layerz in policy_model_std.layers:\n",
    "            if 'dropout' not in layerz.name:\n",
    "                layerz.set_weights([weights[i],weights[i+1]])\n",
    "                i+=2\n",
    "    else:\n",
    "        print('error')\n",
    "\n",
    "def get_value_weights():\n",
    "    weights =[]\n",
    "    for w in value_model.layers:\n",
    "        weights+= w.weights\n",
    "    return weights\n",
    "\n",
    "def get_policy_weights_mean():\n",
    "    weights =[]\n",
    "    for w in policy_model_mean.layers:\n",
    "        weights+= w.weights\n",
    "    return weights\n",
    "\n",
    "def get_policy_weights_std():\n",
    "    weights =[]\n",
    "    for w in policy_model_std.layers:\n",
    "        weights+= w.weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_action(action):\n",
    "    #FIXING ACTION INCONSISTENCIES\n",
    "    low = env.action_space.low\n",
    "    high = env.action_space.high\n",
    "    \n",
    "    for i in range(action.shape[0]):\n",
    "        if(action[i]>high[i]):\n",
    "            return True\n",
    "        if(action[i]<low[i]):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"weights\"):\n",
    "    \n",
    "    with tf.name_scope(\"VALUE\"):\n",
    "        value_weights = get_value_weights()\n",
    "        variable_summaries(value_weights)\n",
    "    with tf.name_scope(\"MEAN\"):\n",
    "        policy_weights_mean = get_policy_weights_mean()\n",
    "        variable_summaries(policy_weights_mean)\n",
    "    with tf.name_scope(\"STD\"):\n",
    "        policy_weights_std = get_policy_weights_std()\n",
    "        variable_summaries(policy_weights_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Rewards\"):\n",
    "    with tf.name_scope(\"R\"):\n",
    "        R = tf.Variable(0, dtype= tf.float32)\n",
    "        variable_summaries2(R)\n",
    "    with tf.name_scope(\"Delta_R\"):\n",
    "        Delta_R = tf.Variable(0, dtype= tf.float32)\n",
    "        variable_summaries2(Delta_R)\n",
    "    with tf.name_scope(\"Average_R\"):\n",
    "        Average_R = tf.Variable(0, dtype= tf.float32)\n",
    "        variable_summaries2(Average_R)\n",
    "    \n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALUE OPS\n",
    "global value_eligibility_traces , value_weights , policy_weights_std\n",
    "value_gradients = value_model.optimizer.get_gradients(value_model.output, value_model.trainable_weights)\n",
    "Delta_rewards = tf.placeholder(tf.float32)\n",
    "value_gradients = [tf.clip_by_norm(gradient, 10) for gradient in value_gradients]\n",
    "value_eligibility_traces_op = [value_eligibility_traces[i].assign(tf.add(tf.multiply(value_eligibility_traces[i], tf.constant(value_lambda, dtype = tf.float32)),value_gradients[i] )) for i in range(len(value_eligibility_traces))] \n",
    "sess.run(tf.global_variables_initializer())\n",
    "value_weights_op =  [value_weights[i].assign(tf.add(value_weights[i], tf.multiply(value_alpha*Delta_rewards, value_eligibility_traces[i]))) for i in range(len(value_weights))]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEAN OPS\n",
    "global policy_eligibility_traces_mean, policy_eligibility_traces_std , policy_weights_mean\n",
    "Action = tf.placeholder(tf.float32, shape = A.shape)\n",
    "Policy_std = tf.placeholder(tf.float32)\n",
    "loss_mean = tf.divide(tf.subtract(Action, policy_model_mean.output), Policy_std*Policy_std)\n",
    "policy_gradients_mean = policy_model_mean.optimizer.get_gradients(loss_mean, policy_model_mean.trainable_weights)\n",
    "policy_gradients_mean = [tf.clip_by_norm(gradient, 10) for gradient in policy_gradients_mean]\n",
    "policy_eligibility_traces_op_mean = [policy_eligibility_traces_mean[i].assign(tf.add(tf.multiply(tf.constant(policy_lambda, dtype = tf.float32), policy_eligibility_traces_mean[i]), policy_gradients_mean[i])) for i in range(len(policy_gradients_mean))]\n",
    "\n",
    "policy_weights_op_mean =  [policy_weights_mean[i].assign(tf.add(policy_weights_mean[i], tf.multiply(policy_eligibility_traces_mean[i],policy_alpha*Delta_rewards))) for i in range(len(policy_weights_mean))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STD OPS\n",
    "global policy_eligibility_traces_mean, policy_eligibility_traces_std , policy_weights_mean\n",
    "Policy_mean =tf.placeholder(tf.float32)\n",
    "loss_std = 2* tf.subtract(tf.divide(tf.square(tf.subtract(Action,Policy_mean)), tf.pow(policy_model_std.output, 3)), tf.divide(1, policy_model_std.output))\n",
    "policy_gradients_std = policy_model_std.optimizer.get_gradients(loss_std, policy_model_std.trainable_weights)\n",
    "policy_gradients_std = [tf.clip_by_norm(gradient, 10) for gradient in policy_gradients_std]\n",
    "policy_eligibility_traces_op_std = [policy_eligibility_traces_std[i].assign(tf.add(tf.multiply(tf.constant(policy_lambda, dtype = tf.float32), policy_eligibility_traces_std[i]), policy_gradients_std[i])) for i in range(len(policy_gradients_std))]\n",
    "\n",
    "policy_weights_op_std =  [policy_weights_std[i].assign(tf.add(policy_weights_std[i], tf.multiply( policy_eligibility_traces_std[i],policy_alpha*Delta_rewards))) for i in range(len(policy_weights_std))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(previous_state, action , reward, state,  terminal, skip_count):\n",
    "    \n",
    "    #Compute rewards\n",
    "    global average_reward,rewardz_received,delta_rewardz_received, cumulative_reward \n",
    "    delta_rewards = reward - average_reward*skip_count + value_model.predict(np.array([state]))[0][0] - value_model.predict(np.array([previous_state]))[0][0]    \n",
    "    cumulative_reward = (cumulative_reward + reward)/2\n",
    "    print(value_model.predict(np.array([previous_state]))[0][0] ,\"=====\",value_model.predict(np.array([state]))[0][0],\"====\",delta_rewards)\n",
    "    if(terminal):\n",
    "        delta_rewards = reward - average_reward*skip_count + 0  - value_model.predict(np.array([previous_state]))[0][0]\n",
    "    thresh = average_reward + cumulative_reward\n",
    "    if(delta_rewards>=thresh):\n",
    "        delta_rewards =thresh\n",
    "    if(delta_rewards<=-thresh):\n",
    "        delta_rewards=-thresh\n",
    "    average_reward = average_reward + etaa*delta_rewards\n",
    "    sess.run(Delta_R.assign(delta_rewards))\n",
    "    sess.run(Average_R.assign(average_reward))\n",
    "    \n",
    "    #Compute value updates (eligibility traces and weights)\n",
    "    global value_eligibility_traces , value_weights , policy_weights_std\n",
    "    sess.run(value_eligibility_traces_op,feed_dict={value_model.input:np.array([previous_state])})    \n",
    "    sess.run(value_weights_op, feed_dict={Delta_rewards:delta_rewards})\n",
    "    \n",
    "    #Compute policy updates (eligibility traces and weights)\n",
    "    \n",
    "    global policy_eligibility_traces_mean, policy_eligibility_traces_std , policy_weights_mean\n",
    "    policy_mean = policy_model_mean.predict(np.array([previous_state]))[0]\n",
    "    policy_std = policy_model_std.predict(np.array([previous_state]))[0]\n",
    "    policy_std= policy_std*policy_std\n",
    "    \n",
    "    ##################################################  MEAN ##################################################################\n",
    "    sess.run(policy_eligibility_traces_op_mean, feed_dict={policy_model_mean.input:np.array([previous_state]), Action:action, Policy_std:policy_std})\n",
    "    sess.run(policy_weights_op_mean, feed_dict={Delta_rewards:delta_rewards})    \n",
    "    \n",
    "    ##################################################   STD ###################################################################\n",
    "    sess.run(policy_eligibility_traces_op_std, feed_dict={policy_model_std.input:np.array([previous_state]), Action:action, Policy_mean:policy_mean})\n",
    "    sess.run(policy_weights_op_std, feed_dict={Delta_rewards:delta_rewards} )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    \n",
    "    mean = policy_model_mean.predict(np.array([state]))[0]\n",
    "    std = policy_model_std.predict(np.array([state]))[0]\n",
    "    std= std*std\n",
    "    #print(mean)\n",
    "    covariance = np.zeros([mean.shape[0],mean.shape[0]])\n",
    "    \n",
    "    for i in range(std.shape[0]):\n",
    "        covariance[i][i]= std[i]\n",
    "        \n",
    "    action = np.random.multivariate_normal(mean, covariance)\n",
    "    #print(action)\n",
    "\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'      \\n    #FIXING ACTION INCONSISTENCIES\\n    low = env.action_space.low\\n    high = env.action_space.high\\n    \\n    for i in range(action.shape[0]):\\n        if(action[i]>high[i]):\\n            action[i]= high[i]+0.1\\n        if(action[i]<low[i]):\\n            action[i]=low[i]+0.1\\n            \\\\    high = env.action_space.high\\n    global average_reward_variance\\n    print(average_reward_variance)\\n    if(np.abs(average_reward_variance)<0.1):\\n        print(\"Sampling randomly\")\\n        return env.action_space.sample()\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''      \n",
    "    #FIXING ACTION INCONSISTENCIES\n",
    "    low = env.action_space.low\n",
    "    high = env.action_space.high\n",
    "    \n",
    "    for i in range(action.shape[0]):\n",
    "        if(action[i]>high[i]):\n",
    "            action[i]= high[i]+0.1\n",
    "        if(action[i]<low[i]):\n",
    "            action[i]=low[i]+0.1\n",
    "            \\    high = env.action_space.high\n",
    "    global average_reward_variance\n",
    "    print(average_reward_variance)\n",
    "    if(np.abs(average_reward_variance)<0.1):\n",
    "        print(\"Sampling randomly\")\n",
    "        return env.action_space.sample()\n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Episodes\n",
    "## Setting up variables :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S = env.reset()\n",
    "#action_count=0\n",
    "episode_count=0\n",
    "save= True\n",
    "total_reward =0 \n",
    "total_reward_list=[]\n",
    "skip_count = 0\n",
    "skip_reward=0\n",
    "action_count= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./logz/Ep_\"+str(episode_count),sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00306105 ===== 0.00198121 ==== -0.00626682121757\n",
      "0.00186252 ===== 0.00220574 ==== -0.00470463423446\n",
      "0.00203514 ===== 0.00131497 ==== -0.005737339335\n",
      "0.00101063 ===== 0.00448499 ==== -0.00145667718147\n",
      "0.00438768 ===== 0.00122808 ==== -0.00809372549603\n",
      "0.000555771 ===== 0.00117759 ==== -0.00423354392281\n",
      "0.000770394 ===== 0.00228196 ==== -0.00329660106995\n",
      "0.00192524 ===== 0.00301059 ==== -0.00372170150626\n",
      "0.0025842 ===== 7.7799e-05 ==== -0.00727770635137\n",
      "-0.000833003 ===== 0.638015 ==== -2.36924274167\n",
      "-5.24348 ===== -2.85126 ==== 1.46053485979\n",
      "9.03564 ===== 3.25747 ==== -5.95263975076\n",
      "-26.2483 ===== -30.5881 ==== -4.60218055109\n",
      "-1463.94 ===== -390.634 ==== 1073.32778198\n",
      "0.512097 ===== 0.244645 ==== -0.224686574316\n",
      "0.169815 ===== 0.233401 ==== 0.0827354514953\n",
      "0.273866 ===== 0.236873 ==== 0.0198842075535\n",
      "0.250019 ===== 0.206593 ==== -0.0154491473603\n",
      "0.194181 ===== 0.493668 ==== 0.268058221848\n",
      "0.841865 ===== 0.460818 ==== -0.35012856278\n",
      "0.0699725 ===== 0.0882595 ==== 0.0701195649835\n",
      "0.158711 ===== 0.0269741 ==== -0.0918985864587\n",
      "-0.0445985 ===== 1.45857 ==== 0.574028173067\n",
      "4.59921 ===== 2.10372 ==== -2.64150882509\n",
      "-1.84571 ===== -2.70517 ==== -1.15645780001\n",
      "-15.7635 ===== -6.84221 ==== 8.7745688338\n",
      "65.6469 ===== 19.2859 ==== -46.1657586515\n",
      "-3.23373 ===== -28.0949 ==== -25.0756445749\n",
      "-35616.3 ===== -26721.5 ==== 8894.69058788\n",
      "-0.574567 ===== -0.579388 ==== -0.264651758198\n",
      "-0.682453 ===== -0.71043 ==== -0.150894418827\n",
      "-0.868139 ===== -0.894876 ==== -0.0513522847661\n",
      "-0.923384 ===== -0.725025 ==== 0.303055296697\n",
      "-0.637453 ===== -0.665606 ==== -0.748078934195\n",
      "-1.99639 ===== -1.3469 ==== 0.349723414158\n",
      "-0.743264 ===== -0.963543 ==== -0.272232548114\n",
      "-1.25479 ===== -0.826035 ==== 0.572287945248\n",
      "-0.620225 ===== -0.719312 ==== 0.0331996421917\n",
      "-0.708735 ===== -0.630822 ==== 0.235753049468\n",
      "-0.552355 ===== -0.693172 ==== 0.00149711299883\n",
      "-0.692726 ===== -0.542936 ==== 0.295822982528\n",
      "-0.469382 ===== -0.437465 ==== 0.169963009318\n",
      "-0.397546 ===== -0.398025 ==== -0.92261517291\n",
      "-0.654047 ===== -0.689087 ==== 0.191174100945\n",
      "-0.609586 ===== -0.722645 ==== 0.151119390513\n",
      "-0.679013 ===== -0.581482 ==== 0.356039989551\n",
      "-0.481993 ===== -0.523245 ==== 0.251298883131\n",
      "-0.455533 ===== -0.20634 ==== 0.427049708454\n",
      "0.145049 ===== -0.133462 ==== -0.0575948207325\n",
      "-0.230484 ===== -0.331526 ==== 0.14359449759\n",
      "-0.283941 ===== -0.3366 ==== 0.190431775238\n",
      "-0.280139 ===== -0.110042 ==== 0.411171033312\n",
      "0.11039 ===== 0.121755 ==== 0.250100670162\n",
      "0 --total reward =257.929143311\n",
      "0 --average reward =4.94986627477\n",
      "saving models\n",
      "average reward 4.94986627477\n",
      "0.00892242 ===== 0.00916243 ==== 0.0368034747755\n",
      "0.00990403 ===== 0.011926 ==== 0.0382025809737\n",
      "0.0133711 ===== 0.00824333 ==== 0.030663877071\n",
      "0.0098668 ===== 0.012518 ==== 0.0381256247389\n",
      "0.0150604 ===== 0.0165068 ==== 0.0362639392296\n",
      "0.0193332 ===== 0.0220072 ==== 0.0371406528037\n",
      "0.0253787 ===== 0.0271014 ==== 0.0355333114855\n",
      "0.0306942 ===== 0.0355809 ==== 0.0382203510811\n",
      "0.0397455 ===== 0.0436704 ==== 0.0360048546051\n",
      "0.0481062 ===== 0.046894 ==== 0.0307779505952\n",
      "0.0509712 ===== 2.99157 ==== 0.987076681155\n",
      "5.73719 ===== 1.87312 ==== -4.09589837759\n",
      "-17.9962 ===== -15.5963 ==== 2.2557234701\n",
      "-0.0285873 ===== -0.363555 ==== -0.238509881138\n",
      "-0.646608 ===== -0.339819 ==== 0.466185662629\n",
      "0.125709 ===== 0.297043 ==== 0.303380380136\n",
      "0.699683 ===== 0.439528 ==== -0.0888298492525\n",
      "0.298452 ===== 0.196022 ==== 0.121386055037\n",
      "0.309296 ===== 0.235328 ==== 0.177001185839\n",
      "0.360202 ===== 0.303549 ==== 0.195963640425\n",
      "0.44534 ===== 3.6657 ==== 3.287674853\n",
      "65.2521 ===== 16.6045 ==== -48.5668974149\n",
      "-6.09492 ===== -7.96527 ==== -1.72507725452\n",
      "-18.5315 ===== -66.98 ==== -48.4914045048\n",
      "-10381.7 ===== -22111.6 ==== -11730.2744516\n",
      "-907412.0 ===== -615640.0 ==== 291771.995149\n",
      "-250.274 ===== -2014.27 ==== -1763.98303915\n",
      "-346150.0 ===== -157164.0 ==== 188986.488062\n",
      "-474.78 ===== -63.7123 ==== 410.800568215\n",
      "-0.917708 ===== -0.814869 ==== 0.175768957949\n",
      "-0.755573 ===== -0.787339 ==== 0.186092503263\n",
      "-0.766463 ===== -0.821558 ==== 0.159292250287\n",
      "-0.79801 ===== -0.643791 ==== 0.376683454064\n",
      "-0.50072 ===== -0.666618 ==== 0.0473579208269\n",
      "-0.658364 ===== -0.312801 ==== -0.374901178764\n",
      "1 --total reward =170.952325347\n",
      "1 --average reward =4.8728789823\n",
      "saving models\n",
      "average reward 4.8728789823\n",
      "0.00637782 ===== 0.00218702 ==== 0.127312289244\n",
      "0.00457228 ===== 0.0122655 ==== 0.137636232573\n",
      "0.0169653 ===== 0.025032 ==== 0.136500840699\n",
      "0.0319506 ===== 0.0230017 ==== 0.117881023921\n",
      "0.0307819 ===== 0.0365054 ==== 0.130809044916\n",
      "0.0477802 ===== 0.0399337 ==== 0.113888545054\n",
      "0.0508946 ===== 0.0801629 ==== 0.147016565922\n",
      "0.101742 ===== 0.100212 ==== 0.108325646447\n",
      "0.119195 ===== 0.153877 ==== 0.144602789302\n",
      "0.192209 ===== 0.17643 ==== 0.0874700800947\n",
      "0.210673 ===== 0.980582 ==== 0.661865789211\n",
      "2.70673 ===== 1.69773 ==== -0.887606208281\n",
      "-1.39662 ===== -0.070122 ==== 1.61955721173\n",
      "1.05133 ===== 2.99991 ==== 2.14829297093\n",
      "18.1086 ===== 21.2865 ==== 3.19444875908\n",
      "461.232 ===== 436.314 ==== -24.9407556358\n",
      "-1.73863 ===== -1.12456 ==== 0.875593160501\n",
      "-0.652073 ===== -0.304828 ==== 0.645050204031\n",
      "2 --total reward =90.1995893573\n",
      "2 --average reward =4.87179201794\n",
      "saving models\n",
      "average reward 4.87179201794\n",
      "0.00367665 ===== 0.00269025 ==== 0.140691092089\n",
      "0.00527141 ===== 0.00619861 ==== 0.141145700028\n",
      "0.0111191 ===== 0.00760582 ==== 0.1349697074\n",
      "0.0143574 ===== 0.0162982 ==== 0.138029873907\n",
      "0.0248135 ===== 0.0193648 ==== 0.124981809258\n",
      "0.0284111 ===== 0.0352691 ==== 0.138480410763\n",
      "0.0470965 ===== 0.0468289 ==== 0.124267145855\n",
      "0.0590274 ===== 0.0791817 ==== 0.139897031501\n",
      "0.0963699 ===== 0.0910373 ==== 0.114322835328\n",
      "0.107943 ===== 3.09837 ==== -0.309726064787\n",
      "1.93029 ===== 0.775224 ==== -1.53587737148\n",
      "-16.1824 ===== -7.76963 ==== 8.42565127951\n",
      "54.5536 ===== 33.9435 ==== -20.8201388999\n",
      "-6.25366 ===== -7.10471 ==== -0.743569999871\n",
      "-23.7487 ===== -27.814 ==== -4.10203711953\n",
      "-3369.99 ===== -10273.4 ==== -6904.39226052\n",
      "-1.63957e+06 ===== -272295.0 ==== 1367276.00705\n",
      "-54.7723 ===== -19.689 ==== 34.8731408538\n",
      "0.672423 ===== 0.258045 ==== -0.570335845615\n",
      "0.177651 ===== 0.190495 ==== -0.156769367261\n",
      "0.167258 ===== 0.466133 ==== 0.0983940910126\n",
      "0.510089 ===== 0.394782 ==== -0.18002729792\n",
      "0.301465 ===== 0.148656 ==== -0.141021577466\n",
      "0.113235 ===== 0.0452996 ==== -0.0784354211237\n",
      "-0.00619654 ===== 0.182224 ==== 0.17175993118\n",
      "0.377768 ===== 0.319746 ==== -0.0641334085382\n",
      "0.177333 ===== 0.516229 ==== 0.105963148929\n",
      "0.807957 ===== 0.467445 ==== -0.144772263701\n",
      "0.0994621 ===== 0.190446 ==== 0.29213406473\n",
      "0.751847 ===== 0.724083 ==== 0.162083638823\n",
      "1.29648 ===== 2.35293 ==== 1.04411256751\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    \n",
    "    #renders environment \n",
    "    #env.render()\n",
    "    \n",
    "    high = env.action_space.high\n",
    "    #Selects action according to stochastic policy\n",
    "    action = select_action(S%high[0])\n",
    "    summary = sess.run(merged)\n",
    "    writer.add_summary(summary, action_count)\n",
    "    action_count+=1\n",
    "    \n",
    "    \n",
    "    #Takes action \n",
    "    S1, reward, done, info = env.step(action)\n",
    "    skip_reward += reward\n",
    "    skip_count += 1\n",
    "    sess.run(R.assign(reward))\n",
    "    \n",
    "    #Updates weights\n",
    "    #if(np.random.randint(10)%5==0):\n",
    "    if(True):\n",
    "        update_weights(S , action, skip_reward, S1 , done, skip_count)\n",
    "        skip_reward = 0\n",
    "        skip_count =0\n",
    "    \n",
    "    S = S1\n",
    "    total_reward +=reward\n",
    "    \n",
    "    if(done):\n",
    "        \n",
    "        #print('We are now at '+str(episode_count))\n",
    "        gc.collect()\n",
    "        print(str(episode_count)+' --total reward ='+ str(total_reward))\n",
    "        print(str(episode_count)+' --average reward ='+ str(average_reward))\n",
    "        total_reward_list.append(total_reward)\n",
    "        total_reward=0\n",
    "        \n",
    "        #Resets episode\n",
    "        S = env.reset()\n",
    "        #action_count=0\n",
    "        episode_count+=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(episode_count%1==0):\n",
    "            \n",
    "            if(save):\n",
    "                print('saving models')\n",
    "                print('average reward '+ str(average_reward))\n",
    "                \n",
    "                policy_model_mean.save('policy_mean.h5')\n",
    "                policy_model_std.save('policy_std.h5')\n",
    "                value_model.save('value.h5')\n",
    "                pickle.dump(total_reward_list,open('totalz_hist','wb')) \n",
    "                \n",
    "                \n",
    "                #Restarting keras session \n",
    "                '''\n",
    "                K.clear_session()\n",
    "                sess = K.get_session()\n",
    "                policy_model_mean = load_model('policy_mean.h5')\n",
    "                policy_model_std = load_model('policy_std.h5')\n",
    "                value_model = load_model('value.h5')\n",
    "                '''\n",
    "\n",
    "        \n",
    "        #Resets eligibility traces\n",
    "        global value_eligibility_traces, policy_eligibility_traces_mean, policy_eligibility_traces_std\n",
    "        \n",
    "        with tf.name_scope(\"eligibility_traces\"):\n",
    "            with tf.name_scope(\"MEAN\"):\n",
    "                policy_eligibility_traces_mean= [tf.Variable(np.zeros(shape = sess.run(tf.shape(tensor))), dtype= tf.float32) for tensor in policy_model_mean.trainable_weights]\n",
    "                variable_summaries(policy_eligibility_traces_mean)\n",
    "            with tf.name_scope(\"STD\"):\n",
    "                policy_eligibility_traces_std= [tf.Variable(np.zeros(shape = sess.run(tf.shape(tensor))), dtype= tf.float32) for tensor in policy_model_std.trainable_weights]\n",
    "                variable_summaries(policy_eligibility_traces_std)\n",
    "            with tf.name_scope(\"VALUE\"):\n",
    "                value_eligibility_traces= [tf.Variable(np.zeros(shape = sess.run(tf.shape(tensor))), dtype= tf.float32) for tensor in value_model.trainable_weights]\n",
    "                variable_summaries(value_eligibility_traces)\n",
    "\n",
    "        with tf.name_scope(\"weights\"):\n",
    "\n",
    "            with tf.name_scope(\"VALUE\"):\n",
    "                value_weights = get_value_weights()\n",
    "                variable_summaries(value_weights)\n",
    "            with tf.name_scope(\"MEAN\"):\n",
    "                policy_weights_mean = get_policy_weights_mean()\n",
    "                variable_summaries(policy_weights_mean)\n",
    "            with tf.name_scope(\"STD\"):\n",
    "                policy_weights_std = get_policy_weights_std()\n",
    "                variable_summaries(policy_weights_std)\n",
    "        \n",
    "        with tf.name_scope(\"Rewards\"):\n",
    "            with tf.name_scope(\"R\"):\n",
    "                R = tf.Variable(0, dtype= tf.float32)\n",
    "                variable_summaries2(R)\n",
    "            with tf.name_scope(\"Delta_R\"):\n",
    "                Delta_R = tf.Variable(0, dtype= tf.float32)\n",
    "                variable_summaries2(Delta_R)\n",
    "            with tf.name_scope(\"Average_R\"):\n",
    "                Average_R = tf.Variable(0, dtype= tf.float32)\n",
    "                variable_summaries2(Average_R)\n",
    "\n",
    "        #VALUE OPS\n",
    "        value_gradients = value_model.optimizer.get_gradients(value_model.output, value_model.trainable_weights)\n",
    "        Delta_rewards = tf.placeholder(tf.float32)\n",
    "        value_gradients = [tf.clip_by_norm(gradient, 10) for gradient in value_gradients]\n",
    "        value_eligibility_traces_op = [value_eligibility_traces[i].assign(tf.add(tf.multiply(value_eligibility_traces[i], tf.constant(value_lambda, dtype = tf.float32)),value_gradients[i] )) for i in range(len(value_eligibility_traces))] \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        value_weights_op =  [value_weights[i].assign(tf.add(value_weights[i], tf.multiply(value_alpha*Delta_rewards, value_eligibility_traces[i]))) for i in range(len(value_weights))]\n",
    "\n",
    "        #MEAN OPS\n",
    "        Action = tf.placeholder(tf.float32, shape = A.shape)\n",
    "        Policy_std = tf.placeholder(tf.float32)\n",
    "        loss_mean = tf.divide(tf.subtract(Action, policy_model_mean.output), Policy_std*Policy_std)\n",
    "        policy_gradients_mean = policy_model_mean.optimizer.get_gradients(loss_mean, policy_model_mean.trainable_weights)\n",
    "        policy_gradients_mean = [tf.clip_by_norm(gradient, 10) for gradient in policy_gradients_mean]\n",
    "        policy_eligibility_traces_op_mean = [policy_eligibility_traces_mean[i].assign(tf.add(tf.multiply(tf.constant(policy_lambda, dtype = tf.float32), policy_eligibility_traces_mean[i]), policy_gradients_mean[i])) for i in range(len(policy_gradients_mean))]\n",
    "\n",
    "        policy_weights_op_mean =  [policy_weights_mean[i].assign(tf.add(policy_weights_mean[i], tf.multiply(policy_eligibility_traces_mean[i],policy_alpha*Delta_rewards))) for i in range(len(policy_weights_mean))]\n",
    "        \n",
    "        #STD OPS\n",
    "        Policy_mean =tf.placeholder(tf.float32)\n",
    "        loss_std = 2* tf.subtract(tf.divide(tf.square(tf.subtract(Action,Policy_mean)), tf.pow(policy_model_std.output, 3)), tf.divide(1, policy_model_std.output))\n",
    "        policy_gradients_std = policy_model_std.optimizer.get_gradients(loss_std, policy_model_std.trainable_weights)\n",
    "        policy_gradients_std = [tf.clip_by_norm(gradient, 10) for gradient in policy_gradients_std]\n",
    "        policy_eligibility_traces_op_std = [policy_eligibility_traces_std[i].assign(tf.add(tf.multiply(tf.constant(policy_lambda, dtype = tf.float32), policy_eligibility_traces_std[i]), policy_gradients_std[i])) for i in range(len(policy_gradients_std))]\n",
    "\n",
    "        policy_weights_op_std =  [policy_weights_std[i].assign(tf.add(policy_weights_std[i], tf.multiply( policy_eligibility_traces_std[i],policy_alpha*Delta_rewards))) for i in range(len(policy_weights_std))]\n",
    "\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(\"./logz/Ep_\"+str(episode_count),sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
