{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "#from keras.constraints import MaxNorm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import pickle\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/gpu:0']\n"
     ]
    }
   ],
   "source": [
    "#Checking use of GPUs\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "sess= K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializing environment\n",
    "env = gym.make('Humanoid-v2')\n",
    "So = env.reset()\n",
    "A = env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 400)               157600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               40100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 200,761\n",
      "Trainable params: 200,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Creating the model for policy\n",
    "global policy_model\n",
    "policy_model = Sequential()\n",
    "policy_model.add(Dense(400,input_shape = np.append(So, A).shape,kernel_initializer='random_uniform', activation = 'sigmoid' ))\n",
    "policy_model.add(Dropout(0.5))\n",
    "policy_model.add(Dense(100,kernel_initializer='random_uniform', activation = 'sigmoid'))\n",
    "policy_model.add(Dropout(0.5))\n",
    "policy_model.add(Dense(30,kernel_initializer='random_uniform', activation = 'sigmoid'))\n",
    "policy_model.add(Dense(1,kernel_initializer='random_uniform', activation= 'sigmoid'))\n",
    "\n",
    "#adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "policy_model.compile(loss=\"mse\", optimizer= \"sgd\")\n",
    "print(policy_model.summary())\n",
    "policy_model.load_weights(\"policy.h5\")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#Intializing eligibility traces for policy improvement\n",
    "global policy_eligibility_traces\n",
    "policy_eligibility_traces= [tf.zeros(shape = tensor.eval(session = sess).shape) for tensor in policy_model.trainable_weights]\n",
    "\n",
    "#Setting learning hyperparameters \n",
    "global policy_alpha\n",
    "policy_alpha = 0.1\n",
    "global policy_lambda\n",
    "policy_lambda =  0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 400)               150800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               40100     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 193,961\n",
      "Trainable params: 193,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Creating model for values \n",
    "global value_model\n",
    "value_model = Sequential()\n",
    "value_model.add(Dense(400,input_shape = So.shape, kernel_initializer='random_uniform',activation = 'relu' ))\n",
    "value_model.add(Dropout(0.5))\n",
    "value_model.add(Dense(100,kernel_initializer='random_uniform', activation = 'relu'))\n",
    "value_model.add(Dropout(0.5))\n",
    "value_model.add(Dense(30, kernel_initializer='random_uniform',activation = 'relu'))\n",
    "value_model.add(Dense(1,kernel_initializer='random_uniform', activation= 'linear'))\n",
    "\n",
    "#adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "value_model.compile(loss=\"mse\", optimizer= \"sgd\")\n",
    "print(value_model.summary())\n",
    "value_model.load_weights(\"value.h5\")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#Intializing eligibility traces for value improvement\n",
    "global value_eligibility_traces\n",
    "value_eligibility_traces= [tf.zeros(shape = tensor.eval(session = sess).shape) for tensor in value_model.trainable_weights]\n",
    "\n",
    "#Setting learning hyperparameters \n",
    "global value_alpha\n",
    "value_alpha = 0.1\n",
    "global value_lambda\n",
    "value_lambda = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining other hyperparameters\n",
    "\n",
    "global average_reward\n",
    "average_reward = pickle.load(open('average_reward.ferch','rb'))\n",
    "global etaa\n",
    "etaa = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Auxiliary functions for weights update\n",
    "\n",
    "def set_weights(model, weights):  \n",
    "    if(model=='value'):\n",
    "        i=0\n",
    "        for layer in value_model.layers:\n",
    "            if 'dropout' not in layer.name:\n",
    "                layer.set_weights([weights[i],weights[i+1]])\n",
    "                i+=2\n",
    "    elif(model=='policy'):\n",
    "        i=0\n",
    "        for layerz in policy_model.layers:\n",
    "            if 'dropout' not in layerz.name:\n",
    "                layerz.set_weights([weights[i],weights[i+1]])\n",
    "                i+=2\n",
    "    else:\n",
    "        print('error')\n",
    "\n",
    "def get_value_weights():\n",
    "    weights =[]\n",
    "    for w in value_model.layers:\n",
    "        weights+= w.weights\n",
    "    return weights\n",
    "\n",
    "def get_policy_weights():\n",
    "    weights =[]\n",
    "    for w in policy_model.layers:\n",
    "        weights+= w.weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reward Tracking\n",
    "rewardz_received = []\n",
    "delta_rewardz_received = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Update weights\n",
    "def update_weights(previous_state, action , reward, state,  terminal):\n",
    "    \n",
    "    #Compute rewards\n",
    "    global average_reward,rewardz_received,delta_rewardz_received\n",
    "    delta_rewards = np.exp(reward) - average_reward + value_model.predict(np.array([state]))[0][0] - value_model.predict(np.array([previous_state]))[0][0]    \n",
    "    rewardz_received.append(reward)\n",
    "    delta_rewardz_received.append(delta_rewards)\n",
    "    if(terminal):\n",
    "        delta_rewards = reward - average_reward + 0 - value_model.predict(np.array([previous_state]))[0][0]\n",
    "    if(delta_rewards>=10):\n",
    "        delta_rewards =10\n",
    "    if(delta_rewards<=-10):\n",
    "        delta_rewards=-10\n",
    "    average_reward = average_reward + etaa*delta_rewards\n",
    "    \n",
    "    \n",
    "    #Compute value updates (eligibility traces and weights)\n",
    "    global value_eligibility_traces\n",
    "    value_gradients = value_model.optimizer.get_gradients(value_model.output, value_model.trainable_weights)\n",
    "    value_gradients = [tf.clip_by_norm(gradient, 10) for gradient in value_gradients]\n",
    "    value_eligibility_traces_op = [tf.add(tf.multiply(value_eligibility_traces[i], tf.constant(value_lambda,dtype= tf.float32)),value_gradients[i] ) for i in range(len(value_eligibility_traces))] \n",
    "    value_eligibility_traces = [tf.convert_to_tensor(x) for x in sess.run(value_eligibility_traces_op,feed_dict={value_model.input:np.array([previous_state])})]       \n",
    "    value_weights = get_value_weights()\n",
    "    value_weights_op =  [tf.add(value_weights[i], tf.multiply(tf.constant(value_alpha*delta_rewards,dtype=tf.float32), value_eligibility_traces[i])) for i in range(len(value_weights))]\n",
    "    set_weights('value', sess.run(value_weights_op))\n",
    "    \n",
    "    #Compute policy updates (eligibility traces and weights)\n",
    "    global policy_eligibility_traces\n",
    "    policy_gradients = policy_model.optimizer.get_gradients(policy_model.output, policy_model.trainable_weights)\n",
    "    policy_gradients = [tf.clip_by_norm(gradient, 10) for gradient in policy_gradients]\n",
    "    policy_value = policy_model.predict(np.array([np.append(previous_state, action)]))[0][0]\n",
    "    policy_value = 1/ (policy_value+1)\n",
    "    policy_eligibility_traces_op = [tf.add(tf.multiply(tf.constant(policy_lambda,dtype=tf.float32), policy_eligibility_traces[i]), tf.multiply(tf.constant(policy_value, dtype= tf.float32), policy_gradients[i])) for i in range(len(policy_gradients))]\n",
    "    policy_eligibility_traces = [tf.convert_to_tensor(x) for x in sess.run(policy_eligibility_traces_op, feed_dict={policy_model.input:np.array([np.append(previous_state,action)])})]\n",
    "    policy_weights = get_policy_weights()\n",
    "    policy_weights_op =  [tf.add(policy_weights[i], tf.multiply( policy_eligibility_traces[i],tf.constant(policy_alpha*delta_rewards,dtype=tf.float32))) for i in range(len(policy_weights))]\n",
    "    set_weights('policy', sess.run(policy_weights_op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 400)               150800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               40100     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 170)               5270      \n",
      "=================================================================\n",
      "Total params: 199,200\n",
      "Trainable params: 199,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Creating model for remembering preferences over each state\n",
    "number_of_preferences =10\n",
    "global preference_model\n",
    "preference_model = Sequential()\n",
    "preference_model.add(Dense(400,input_shape = So.shape, activation = 'relu' ))\n",
    "preference_model.add(Dropout(0.5))\n",
    "preference_model.add(Dense(100, activation = 'relu'))\n",
    "preference_model.add(Dropout(0.5))\n",
    "preference_model.add(Dense(30, activation = 'relu'))\n",
    "preference_model.add(Dense(A.shape[0]*number_of_preferences, activation= 'linear'))\n",
    "\n",
    "adam = Adam(lr=0.4, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "preference_model.compile(loss=\"mse\", optimizer=adam)\n",
    "print(preference_model.summary())\n",
    "preference_model.load_weights('preference.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Selecting an action\n",
    "def select_action(state):\n",
    "    \n",
    "    number_of_new_actions = 3\n",
    "    \n",
    "    predicted_preferences = preference_model.predict(np.array([state]))[0].reshape((number_of_preferences,A.shape[0]))\n",
    "    \n",
    "    #FIXING MATRIX INCONSISTENCIES\n",
    "    low = env.action_space.low\n",
    "    high = env.action_space.high\n",
    "    \n",
    "    for y in range(predicted_preferences.shape[0]):\n",
    "        for z in range(predicted_preferences.shape[1]):\n",
    "            if(predicted_preferences[y][z] < low[z]):\n",
    "                predicted_preferences[y][z] = low[z]\n",
    "            if(predicted_preferences[y][z] > high[z]):\n",
    "                predicted_preferences[y][z] = high[z]\n",
    "                \n",
    "    new_actions = env.action_space.sample().reshape(1, A.shape[0])\n",
    "    \n",
    "    for _ in range(number_of_new_actions-1):\n",
    "        new_actions= np.concatenate((new_actions,env.action_space.sample().reshape(1, A.shape[0])), axis=0 )\n",
    "    \n",
    "    all_actions = np.concatenate((predicted_preferences, new_actions), axis=0)\n",
    "    all_actionz = np.concatenate((np.tile(So, (all_actions.shape[0], 1)), all_actions),axis=1)\n",
    "    policy_preferences = policy_model.predict(all_actionz)\n",
    "    #print(policy_preferences)\n",
    "    \n",
    "    actions_list = []\n",
    "    for i in range(all_actions.shape[0]):\n",
    "        actions_list.append((all_actions[i], policy_preferences[i][0]))\n",
    "    actions_list = sorted(actions_list, key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    probabilities = sess.run(tf.contrib.layers.softmax(tf.constant([[x[1] for x in actions_list]])))[0]\n",
    "    p = np.random.rand()\n",
    "    i=0\n",
    "    #print(probabilities)\n",
    "    for j in range(probabilities.shape[0]):\n",
    "        p-=probabilities[j]\n",
    "        if(p<0):\n",
    "            i=j\n",
    "    \n",
    "    best_actions = np.array([x[0] for x in actions_list[0:number_of_preferences]]).flatten()\n",
    "    preference_model.fit(np.array([state]), np.array([best_actions]), verbose=0)\n",
    "    return actions_list[j][0]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n",
      "We are now at 0\n",
      "total reward 109.113840109\n",
      "We are now at 1\n",
      "total reward 242.31404461\n",
      "We are now at 2\n",
      "total reward 97.5434779521\n",
      "We are now at 3\n",
      "total reward 109.121320867\n",
      "We are now at 4\n",
      "total reward 112.92640442\n",
      "saving models\n",
      "average reward 135.011564136\n",
      "We are now at 5\n",
      "total reward 101.260694043\n",
      "We are now at 6\n",
      "total reward 140.302279725\n",
      "We are now at 7\n",
      "total reward 95.9761549012\n",
      "We are now at 8\n",
      "total reward 93.7178021922\n",
      "We are now at 9\n",
      "total reward 82.7045732193\n",
      "saving models\n",
      "average reward 133.90937266\n",
      "We are now at 10\n",
      "total reward 78.262124873\n",
      "We are now at 11\n",
      "total reward 110.171949576\n",
      "We are now at 12\n",
      "total reward 114.799557196\n",
      "We are now at 13\n",
      "total reward 184.885431268\n",
      "We are now at 14\n",
      "total reward 82.7750815417\n",
      "saving models\n",
      "average reward 132.799879099\n",
      "We are now at 15\n",
      "total reward 143.321806366\n",
      "We are now at 16\n",
      "total reward 100.672591852\n",
      "We are now at 17\n",
      "total reward 105.620675061\n",
      "We are now at 18\n",
      "total reward 116.473664581\n",
      "We are now at 19\n",
      "total reward 88.4094058544\n",
      "saving models\n",
      "average reward 133.40158337\n",
      "We are now at 20\n",
      "total reward 141.182298846\n",
      "We are now at 21\n",
      "total reward 124.782396162\n",
      "We are now at 22\n",
      "total reward 102.326606038\n",
      "We are now at 23\n",
      "total reward 110.95423565\n",
      "We are now at 24\n",
      "total reward 94.3205567124\n",
      "saving models\n",
      "average reward 133.533375873\n",
      "We are now at 25\n",
      "total reward 220.135870552\n",
      "We are now at 26\n",
      "total reward 147.129399871\n",
      "We are now at 27\n",
      "total reward 135.441105267\n",
      "We are now at 28\n",
      "total reward 87.9891236102\n",
      "We are now at 29\n",
      "total reward 95.6701620253\n",
      "saving models\n",
      "average reward 134.639690216\n",
      "We are now at 30\n",
      "total reward 131.087343588\n",
      "We are now at 31\n",
      "total reward 155.289836321\n",
      "We are now at 32\n",
      "total reward 119.244836302\n",
      "We are now at 33\n",
      "total reward 107.420955265\n",
      "We are now at 34\n",
      "total reward 104.895911657\n",
      "saving models\n",
      "average reward 134.321516978\n",
      "We are now at 35\n",
      "total reward 108.873191619\n",
      "We are now at 36\n",
      "total reward 87.7059700295\n",
      "We are now at 37\n",
      "total reward 87.3350923355\n",
      "We are now at 38\n",
      "total reward 99.2706958397\n",
      "We are now at 39\n",
      "total reward 197.723668037\n",
      "saving models\n",
      "average reward 134.695997517\n",
      "We are now at 40\n",
      "total reward 125.193327134\n",
      "We are now at 41\n",
      "total reward 226.935579927\n",
      "We are now at 42\n",
      "total reward 88.3632149126\n",
      "We are now at 43\n",
      "total reward 98.1602277756\n",
      "We are now at 44\n",
      "total reward 106.081328625\n",
      "saving models\n",
      "average reward 133.863891117\n",
      "We are now at 45\n",
      "total reward 123.755164273\n",
      "We are now at 46\n",
      "total reward 134.962548215\n",
      "We are now at 47\n",
      "total reward 245.577122403\n",
      "We are now at 48\n",
      "total reward 78.3773622023\n",
      "We are now at 49\n",
      "total reward 117.286435823\n",
      "saving models\n",
      "average reward 138.243522726\n",
      "We are now at 50\n",
      "total reward 125.030054873\n",
      "We are now at 51\n",
      "total reward 121.733799274\n",
      "We are now at 52\n",
      "total reward 146.637578988\n",
      "We are now at 53\n",
      "total reward 104.774034164\n",
      "We are now at 54\n",
      "total reward 149.554338318\n",
      "saving models\n",
      "average reward 137.556057954\n",
      "We are now at 55\n",
      "total reward 114.830360275\n",
      "We are now at 56\n",
      "total reward 94.0749387471\n",
      "We are now at 57\n",
      "total reward 106.855313117\n",
      "We are now at 58\n",
      "total reward 94.0943583402\n",
      "We are now at 59\n",
      "total reward 133.403050574\n",
      "saving models\n",
      "average reward 137.971783185\n",
      "We are now at 60\n",
      "total reward 136.91071073\n",
      "We are now at 61\n",
      "total reward 128.993940231\n",
      "We are now at 62\n",
      "total reward 136.65237044\n",
      "We are now at 63\n",
      "total reward 139.080286535\n",
      "We are now at 64\n",
      "total reward 93.4734953273\n",
      "saving models\n",
      "average reward 136.864687512\n",
      "We are now at 65\n",
      "total reward 96.2159075822\n",
      "We are now at 66\n",
      "total reward 103.436854217\n",
      "We are now at 67\n",
      "total reward 111.772179256\n",
      "We are now at 68\n",
      "total reward 112.444709898\n",
      "We are now at 69\n",
      "total reward 132.3476246\n",
      "saving models\n",
      "average reward 136.886728477\n",
      "We are now at 70\n",
      "total reward 96.1032595652\n",
      "We are now at 71\n",
      "total reward 94.1339609039\n",
      "We are now at 72\n",
      "total reward 130.954402404\n",
      "We are now at 73\n",
      "total reward 102.157224735\n",
      "We are now at 74\n",
      "total reward 104.916648023\n",
      "saving models\n",
      "average reward 138.587315316\n",
      "We are now at 75\n",
      "total reward 142.075635896\n",
      "We are now at 76\n",
      "total reward 82.0684417982\n",
      "We are now at 77\n",
      "total reward 106.998490602\n",
      "We are now at 78\n",
      "total reward 98.622563242\n",
      "We are now at 79\n",
      "total reward 116.432726329\n",
      "saving models\n",
      "average reward 137.681022957\n",
      "We are now at 80\n",
      "total reward 157.120285901\n",
      "We are now at 81\n",
      "total reward 109.357945146\n",
      "We are now at 82\n",
      "total reward 91.2503837727\n",
      "We are now at 83\n",
      "total reward 139.626501312\n",
      "We are now at 84\n",
      "total reward 135.112604512\n",
      "saving models\n",
      "average reward 133.594499298\n",
      "We are now at 85\n",
      "total reward 105.767362071\n",
      "We are now at 86\n",
      "total reward 120.346493377\n",
      "We are now at 87\n",
      "total reward 88.3540452919\n",
      "We are now at 88\n",
      "total reward 87.3796968751\n",
      "We are now at 89\n",
      "total reward 142.42882006\n",
      "saving models\n",
      "average reward 131.79950249\n",
      "We are now at 90\n",
      "total reward 149.140855203\n",
      "We are now at 91\n",
      "total reward 180.821448188\n",
      "We are now at 92\n",
      "total reward 89.1479384958\n",
      "We are now at 93\n",
      "total reward 90.7995385678\n",
      "We are now at 94\n",
      "total reward 86.557773871\n",
      "saving models\n",
      "average reward 129.204230432\n",
      "We are now at 95\n",
      "total reward 86.5071718206\n",
      "We are now at 96\n",
      "total reward 88.7795621992\n",
      "We are now at 97\n",
      "total reward 87.1364551482\n",
      "We are now at 98\n",
      "total reward 91.5527433324\n",
      "We are now at 99\n",
      "total reward 84.3387883703\n",
      "saving models\n",
      "average reward 125.016930762\n",
      "We are now at 100\n",
      "total reward 85.6832480661\n",
      "We are now at 101\n",
      "total reward 89.2376650547\n",
      "We are now at 102\n",
      "total reward 89.0847016539\n",
      "We are now at 103\n",
      "total reward 87.0454106876\n",
      "We are now at 104\n",
      "total reward 83.8444623237\n",
      "saving models\n",
      "average reward 121.431677469\n",
      "We are now at 105\n",
      "total reward 88.8730113866\n",
      "We are now at 106\n",
      "total reward 85.0884365542\n",
      "We are now at 107\n",
      "total reward 92.1372856453\n",
      "We are now at 108\n",
      "total reward 84.8581314387\n",
      "We are now at 109\n",
      "total reward 91.5288453094\n",
      "saving models\n",
      "average reward 119.371138009\n",
      "We are now at 110\n",
      "total reward 87.3202737932\n",
      "We are now at 111\n",
      "total reward 89.9762124851\n",
      "We are now at 112\n",
      "total reward 92.6418079622\n",
      "We are now at 113\n",
      "total reward 82.6930269896\n",
      "We are now at 114\n",
      "total reward 84.9485440821\n",
      "saving models\n",
      "average reward 118.098964514\n",
      "We are now at 115\n",
      "total reward 79.6810776004\n",
      "We are now at 116\n",
      "total reward 83.7026046489\n",
      "We are now at 117\n",
      "total reward 87.3195004168\n",
      "We are now at 118\n",
      "total reward 91.1004615709\n",
      "We are now at 119\n",
      "total reward 86.2634634645\n",
      "saving models\n",
      "average reward 117.005444257\n",
      "We are now at 120\n",
      "total reward 86.4946893203\n",
      "We are now at 121\n",
      "total reward 82.0854830136\n",
      "We are now at 122\n",
      "total reward 91.6104814152\n",
      "We are now at 123\n",
      "total reward 89.4253820998\n",
      "We are now at 124\n",
      "total reward 87.0669859749\n",
      "saving models\n",
      "average reward 116.868777732\n",
      "We are now at 125\n",
      "total reward 87.1938518875\n",
      "We are now at 126\n",
      "total reward 91.3282676086\n",
      "We are now at 127\n",
      "total reward 86.3904264722\n",
      "We are now at 128\n",
      "total reward 87.7555003787\n",
      "We are now at 129\n",
      "total reward 87.6862168822\n",
      "saving models\n",
      "average reward 116.353925704\n",
      "We are now at 130\n",
      "total reward 89.205213618\n",
      "We are now at 131\n",
      "total reward 87.3544979323\n",
      "We are now at 132\n",
      "total reward 87.2657138204\n",
      "We are now at 133\n",
      "total reward 87.434636662\n",
      "We are now at 134\n",
      "total reward 84.7393483501\n",
      "saving models\n",
      "average reward 115.273549206\n",
      "We are now at 135\n",
      "total reward 87.7896414119\n",
      "We are now at 136\n",
      "total reward 90.3422253196\n",
      "We are now at 137\n",
      "total reward 81.8102854992\n",
      "We are now at 138\n",
      "total reward 90.7643289105\n",
      "We are now at 139\n",
      "total reward 91.4712517218\n",
      "saving models\n",
      "average reward 115.096969565\n",
      "We are now at 140\n",
      "total reward 90.0061988538\n",
      "We are now at 141\n",
      "total reward 89.8926238556\n",
      "We are now at 142\n",
      "total reward 88.7253525327\n",
      "We are now at 143\n",
      "total reward 88.8527088659\n",
      "We are now at 144\n",
      "total reward 82.9868422128\n",
      "saving models\n",
      "average reward 114.988078959\n",
      "We are now at 145\n",
      "total reward 86.0284820667\n",
      "We are now at 146\n",
      "total reward 92.5032686403\n",
      "We are now at 147\n",
      "total reward 87.2753490194\n",
      "We are now at 148\n",
      "total reward 81.7511966443\n",
      "We are now at 149\n",
      "total reward 91.1970001128\n",
      "saving models\n",
      "average reward 114.923058368\n",
      "We are now at 150\n",
      "total reward 87.4455031563\n",
      "We are now at 151\n",
      "total reward 86.8088541304\n",
      "We are now at 152\n",
      "total reward 89.6092504918\n",
      "We are now at 153\n",
      "total reward 86.4376423949\n",
      "We are now at 154\n",
      "total reward 86.4573935376\n",
      "saving models\n",
      "average reward 114.964094493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are now at 155\n",
      "total reward 92.5343928927\n",
      "We are now at 156\n",
      "total reward 83.2632247078\n",
      "We are now at 157\n",
      "total reward 85.0355704533\n",
      "We are now at 158\n",
      "total reward 91.5159002778\n",
      "We are now at 159\n",
      "total reward 89.7506002589\n",
      "saving models\n",
      "average reward 115.254672309\n",
      "We are now at 160\n",
      "total reward 86.1272589926\n",
      "We are now at 161\n",
      "total reward 89.496773242\n",
      "We are now at 162\n",
      "total reward 88.8969438132\n",
      "We are now at 163\n",
      "total reward 85.9780533117\n",
      "We are now at 164\n",
      "total reward 84.6012223026\n",
      "saving models\n",
      "average reward 114.818287167\n",
      "We are now at 165\n",
      "total reward 83.0256401584\n",
      "We are now at 166\n",
      "total reward 89.8721878512\n",
      "We are now at 167\n",
      "total reward 86.5603282693\n",
      "We are now at 168\n",
      "total reward 88.8186866781\n",
      "We are now at 169\n",
      "total reward 91.7811519683\n",
      "saving models\n",
      "average reward 114.813385391\n",
      "We are now at 170\n",
      "total reward 84.3626279384\n",
      "We are now at 171\n",
      "total reward 81.7861255554\n",
      "We are now at 172\n",
      "total reward 83.8643402771\n",
      "We are now at 173\n",
      "total reward 84.60211608\n",
      "We are now at 174\n",
      "total reward 82.6392314761\n",
      "saving models\n",
      "average reward 114.561518305\n",
      "We are now at 175\n",
      "total reward 82.0508394741\n",
      "We are now at 176\n",
      "total reward 84.9466997965\n",
      "We are now at 177\n",
      "total reward 90.5276353568\n",
      "We are now at 178\n",
      "total reward 87.4311670781\n",
      "We are now at 179\n",
      "total reward 86.3586991894\n",
      "saving models\n",
      "average reward 114.277841687\n",
      "We are now at 180\n",
      "total reward 80.3606265261\n",
      "We are now at 181\n",
      "total reward 87.4235760636\n",
      "We are now at 182\n",
      "total reward 82.6481975367\n",
      "We are now at 183\n",
      "total reward 87.4431983272\n",
      "We are now at 184\n",
      "total reward 88.6535971096\n",
      "saving models\n",
      "average reward 113.996867724\n",
      "We are now at 185\n",
      "total reward 89.7899249808\n",
      "We are now at 186\n",
      "total reward 82.994267818\n",
      "We are now at 187\n",
      "total reward 87.4538146695\n",
      "We are now at 188\n",
      "total reward 87.355408568\n",
      "We are now at 189\n",
      "total reward 87.4702934338\n",
      "saving models\n",
      "average reward 114.225895636\n",
      "We are now at 190\n",
      "total reward 89.7697363658\n",
      "We are now at 191\n",
      "total reward 86.3197530378\n",
      "We are now at 192\n",
      "total reward 86.1089931422\n",
      "We are now at 193\n",
      "total reward 82.7534479759\n",
      "We are now at 194\n",
      "total reward 81.4900696268\n",
      "saving models\n",
      "average reward 114.003283372\n",
      "We are now at 195\n",
      "total reward 85.7517237313\n",
      "We are now at 196\n",
      "total reward 84.777792386\n",
      "We are now at 197\n",
      "total reward 92.2776859945\n",
      "We are now at 198\n",
      "total reward 84.7679334424\n",
      "We are now at 199\n",
      "total reward 82.871534476\n",
      "saving models\n",
      "average reward 114.149173144\n",
      "We are now at 200\n",
      "total reward 87.9683255436\n",
      "We are now at 201\n",
      "total reward 87.0589484198\n",
      "We are now at 202\n",
      "total reward 86.6757410554\n",
      "We are now at 203\n",
      "total reward 86.0533433257\n",
      "We are now at 204\n",
      "total reward 86.2931710805\n",
      "saving models\n",
      "average reward 114.246925705\n",
      "We are now at 205\n",
      "total reward 82.8817988311\n",
      "We are now at 206\n",
      "total reward 110.984816345\n",
      "We are now at 207\n",
      "total reward 93.6531084619\n",
      "We are now at 208\n",
      "total reward 66.6593856561\n",
      "We are now at 209\n",
      "total reward 66.6859930634\n",
      "saving models\n",
      "average reward 114.514175791\n",
      "We are now at 210\n",
      "total reward 66.3259194466\n",
      "We are now at 211\n",
      "total reward 71.6231398985\n",
      "We are now at 212\n",
      "total reward 66.9300035803\n",
      "We are now at 213\n",
      "total reward 66.8699993751\n",
      "We are now at 214\n",
      "total reward 66.547310032\n",
      "saving models\n",
      "average reward 115.644303294\n",
      "We are now at 215\n",
      "total reward 66.3924986309\n",
      "We are now at 216\n",
      "total reward 66.721736838\n",
      "We are now at 217\n",
      "total reward 66.3435616064\n",
      "We are now at 218\n",
      "total reward 66.4898036321\n",
      "We are now at 219\n",
      "total reward 66.6140826253\n",
      "saving models\n",
      "average reward 115.701809572\n",
      "We are now at 220\n",
      "total reward 66.4907383425\n",
      "We are now at 221\n",
      "total reward 66.4066271393\n",
      "We are now at 222\n",
      "total reward 66.5301361081\n",
      "We are now at 223\n",
      "total reward 66.3773373448\n",
      "We are now at 224\n",
      "total reward 66.7948525069\n",
      "saving models\n",
      "average reward 115.84133183\n",
      "We are now at 225\n",
      "total reward 66.9020934061\n",
      "We are now at 226\n",
      "total reward 66.7251380651\n",
      "We are now at 227\n",
      "total reward 66.8565191926\n",
      "We are now at 228\n",
      "total reward 66.561916244\n",
      "We are now at 229\n",
      "total reward 66.6438107361\n",
      "saving models\n",
      "average reward 116.188117721\n",
      "We are now at 230\n",
      "total reward 66.8674522011\n",
      "We are now at 231\n",
      "total reward 66.1935101614\n",
      "We are now at 232\n",
      "total reward 66.5783093566\n",
      "We are now at 233\n",
      "total reward 66.5049219766\n",
      "We are now at 234\n",
      "total reward 66.2487604562\n",
      "saving models\n",
      "average reward 116.174457628\n",
      "We are now at 235\n",
      "total reward 66.5224278459\n",
      "We are now at 236\n",
      "total reward 66.7379158366\n",
      "We are now at 237\n",
      "total reward 71.7791612883\n",
      "We are now at 238\n",
      "total reward 67.0173366674\n",
      "We are now at 239\n",
      "total reward 66.2197858532\n",
      "saving models\n",
      "average reward 116.586421287\n",
      "We are now at 240\n",
      "total reward 66.6383525206\n",
      "We are now at 241\n",
      "total reward 66.4096980945\n",
      "We are now at 242\n",
      "total reward 66.755443967\n",
      "We are now at 243\n",
      "total reward 66.8001953255\n",
      "We are now at 244\n",
      "total reward 66.8749205211\n",
      "saving models\n",
      "average reward 116.782187492\n",
      "We are now at 245\n",
      "total reward 71.5024276722\n",
      "We are now at 246\n",
      "total reward 66.9417591978\n",
      "We are now at 247\n",
      "total reward 66.5979281964\n",
      "We are now at 248\n",
      "total reward 66.2038032561\n",
      "We are now at 249\n",
      "total reward 66.2205518626\n",
      "saving models\n",
      "average reward 116.310537473\n",
      "We are now at 250\n",
      "total reward 66.7747762964\n",
      "We are now at 251\n",
      "total reward 66.8886459997\n",
      "We are now at 252\n",
      "total reward 66.3260467858\n",
      "We are now at 253\n",
      "total reward 66.3835809684\n",
      "We are now at 254\n",
      "total reward 66.5511178652\n",
      "saving models\n",
      "average reward 116.268754089\n",
      "We are now at 255\n",
      "total reward 66.7891999333\n",
      "We are now at 256\n",
      "total reward 71.7689852738\n",
      "We are now at 257\n",
      "total reward 66.8859324902\n",
      "We are now at 258\n",
      "total reward 66.4145530785\n",
      "We are now at 259\n",
      "total reward 66.6367071651\n",
      "saving models\n",
      "average reward 116.502820631\n",
      "We are now at 260\n",
      "total reward 71.4868825802\n",
      "We are now at 261\n",
      "total reward 76.0585694495\n",
      "We are now at 262\n",
      "total reward 76.3781934205\n",
      "We are now at 263\n",
      "total reward 76.2332997547\n",
      "We are now at 264\n",
      "total reward 76.1785209898\n",
      "saving models\n",
      "average reward 116.366155919\n",
      "We are now at 265\n",
      "total reward 76.0151778616\n",
      "We are now at 266\n",
      "total reward 74.1151308099\n",
      "We are now at 267\n",
      "total reward 75.8689583925\n",
      "We are now at 268\n",
      "total reward 75.8924094238\n",
      "We are now at 269\n",
      "total reward 76.4277392035\n",
      "saving models\n",
      "average reward 115.645866119\n",
      "We are now at 270\n",
      "total reward 76.1807769274\n",
      "We are now at 271\n",
      "total reward 75.8714850402\n",
      "We are now at 272\n",
      "total reward 78.3811543668\n",
      "We are now at 273\n",
      "total reward 76.3059960552\n",
      "We are now at 274\n",
      "total reward 76.1744547868\n",
      "saving models\n",
      "average reward 115.466762506\n",
      "We are now at 275\n",
      "total reward 76.2363678335\n",
      "We are now at 276\n",
      "total reward 75.7500549518\n",
      "We are now at 277\n",
      "total reward 76.0751136792\n",
      "We are now at 278\n",
      "total reward 70.7860531465\n",
      "We are now at 279\n",
      "total reward 76.3986904209\n",
      "saving models\n",
      "average reward 115.273998227\n",
      "We are now at 280\n",
      "total reward 75.8231927449\n",
      "We are now at 281\n",
      "total reward 78.3781882267\n",
      "We are now at 282\n",
      "total reward 76.3042646121\n",
      "We are now at 283\n",
      "total reward 77.5660332905\n",
      "We are now at 284\n",
      "total reward 76.2791925724\n",
      "saving models\n",
      "average reward 115.473675389\n",
      "We are now at 285\n",
      "total reward 76.0925227636\n",
      "We are now at 286\n",
      "total reward 81.5157071077\n",
      "We are now at 287\n",
      "total reward 85.6146756596\n",
      "We are now at 288\n",
      "total reward 72.2902033996\n",
      "We are now at 289\n",
      "total reward 76.1774590903\n",
      "saving models\n",
      "average reward 115.731583592\n",
      "We are now at 290\n",
      "total reward 76.1422759437\n",
      "We are now at 291\n",
      "total reward 76.248139139\n",
      "We are now at 292\n",
      "total reward 75.8930531461\n",
      "We are now at 293\n",
      "total reward 90.2774202665\n",
      "We are now at 294\n",
      "total reward 77.9314484892\n",
      "saving models\n",
      "average reward 115.612123571\n",
      "We are now at 295\n",
      "total reward 77.678765785\n",
      "We are now at 296\n",
      "total reward 77.6250742843\n",
      "We are now at 297\n",
      "total reward 75.9927972268\n",
      "We are now at 298\n",
      "total reward 76.182520633\n",
      "We are now at 299\n",
      "total reward 72.1437257241\n",
      "saving models\n",
      "average reward 115.797031454\n",
      "We are now at 300\n",
      "total reward 76.1615912233\n",
      "We are now at 301\n",
      "total reward 76.8628584077\n",
      "We are now at 302\n",
      "total reward 76.1680170421\n",
      "We are now at 303\n",
      "total reward 83.6522434968\n",
      "We are now at 304\n",
      "total reward 76.1359296131\n",
      "saving models\n",
      "average reward 115.932211559\n",
      "We are now at 305\n",
      "total reward 76.0160371843\n",
      "We are now at 306\n",
      "total reward 80.9999955048\n",
      "We are now at 307\n",
      "total reward 79.3040312326\n",
      "We are now at 308\n",
      "total reward 76.3239626872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are now at 309\n",
      "total reward 75.8858267736\n",
      "saving models\n",
      "average reward 115.814182883\n",
      "We are now at 310\n",
      "total reward 76.2446209641\n",
      "We are now at 311\n",
      "total reward 69.4618817627\n",
      "We are now at 312\n",
      "total reward 77.7786546099\n",
      "We are now at 313\n",
      "total reward 76.0694897432\n",
      "We are now at 314\n",
      "total reward 79.1164403003\n",
      "saving models\n",
      "average reward 115.637716688\n",
      "We are now at 315\n",
      "total reward 76.2059914211\n",
      "We are now at 316\n",
      "total reward 88.1069637933\n",
      "We are now at 317\n",
      "total reward 82.9912371407\n",
      "We are now at 318\n",
      "total reward 71.9313250514\n",
      "We are now at 319\n",
      "total reward 76.0965174798\n",
      "saving models\n",
      "average reward 115.889197219\n",
      "We are now at 320\n",
      "total reward 76.1418838147\n",
      "We are now at 321\n",
      "total reward 83.0376803688\n",
      "We are now at 322\n",
      "total reward 85.4666884507\n",
      "We are now at 323\n",
      "total reward 82.9631009401\n",
      "We are now at 324\n",
      "total reward 79.1130534499\n",
      "saving models\n",
      "average reward 115.935490893\n",
      "We are now at 325\n",
      "total reward 71.2767885554\n",
      "We are now at 326\n",
      "total reward 76.0417644992\n",
      "We are now at 327\n",
      "total reward 88.4974949253\n",
      "We are now at 328\n",
      "total reward 76.0492459288\n",
      "We are now at 329\n",
      "total reward 77.4434628432\n",
      "saving models\n",
      "average reward 116.250681474\n",
      "We are now at 330\n",
      "total reward 87.9707487907\n",
      "We are now at 331\n",
      "total reward 74.6654156212\n",
      "We are now at 332\n",
      "total reward 74.0768386095\n",
      "We are now at 333\n",
      "total reward 76.1386605386\n",
      "We are now at 334\n",
      "total reward 76.2145639178\n",
      "saving models\n",
      "average reward 116.333885691\n",
      "We are now at 335\n",
      "total reward 75.9611780301\n",
      "We are now at 336\n",
      "total reward 75.9673531582\n",
      "We are now at 337\n",
      "total reward 75.9644156399\n",
      "We are now at 338\n",
      "total reward 76.1253544266\n",
      "We are now at 339\n",
      "total reward 72.4439612147\n",
      "saving models\n",
      "average reward 116.239481713\n",
      "We are now at 340\n",
      "total reward 76.0018144648\n",
      "We are now at 341\n",
      "total reward 76.703255969\n",
      "We are now at 342\n",
      "total reward 81.3443879796\n",
      "We are now at 343\n",
      "total reward 78.4408692115\n",
      "We are now at 344\n",
      "total reward 85.2510502237\n",
      "saving models\n",
      "average reward 116.574377021\n",
      "We are now at 345\n",
      "total reward 73.2847354305\n",
      "We are now at 346\n",
      "total reward 83.4630441007\n",
      "We are now at 347\n",
      "total reward 66.566807688\n",
      "We are now at 348\n",
      "total reward 76.1009886319\n",
      "We are now at 349\n",
      "total reward 76.0273401698\n",
      "saving models\n",
      "average reward 116.289902018\n",
      "We are now at 350\n",
      "total reward 76.7912297659\n",
      "We are now at 351\n",
      "total reward 78.0616244928\n",
      "We are now at 352\n",
      "total reward 82.0356442576\n",
      "We are now at 353\n",
      "total reward 70.3186269685\n",
      "We are now at 354\n",
      "total reward 76.1090753029\n",
      "saving models\n",
      "average reward 116.414622534\n",
      "We are now at 355\n",
      "total reward 71.9489651657\n",
      "We are now at 356\n",
      "total reward 75.9957386608\n",
      "We are now at 357\n",
      "total reward 72.7314354435\n",
      "We are now at 358\n",
      "total reward 76.1323405225\n",
      "We are now at 359\n",
      "total reward 75.9847815377\n",
      "saving models\n",
      "average reward 116.294464267\n",
      "We are now at 360\n",
      "total reward 78.2823468674\n",
      "We are now at 361\n",
      "total reward 75.8232818443\n",
      "We are now at 362\n",
      "total reward 76.1756817995\n",
      "We are now at 363\n",
      "total reward 76.0586497306\n",
      "We are now at 364\n",
      "total reward 75.8344873336\n",
      "saving models\n",
      "average reward 116.084977255\n",
      "We are now at 365\n",
      "total reward 74.2767613538\n",
      "We are now at 366\n",
      "total reward 76.3801829958\n",
      "We are now at 367\n",
      "total reward 76.007040155\n",
      "We are now at 368\n",
      "total reward 83.4110693406\n",
      "We are now at 369\n",
      "total reward 84.1879037487\n",
      "saving models\n",
      "average reward 116.234655121\n",
      "We are now at 370\n",
      "total reward 76.0644265575\n",
      "We are now at 371\n",
      "total reward 82.9668699289\n",
      "We are now at 372\n",
      "total reward 76.1536434562\n",
      "We are now at 373\n",
      "total reward 83.7331243209\n",
      "We are now at 374\n",
      "total reward 75.8291915995\n",
      "saving models\n",
      "average reward 116.128509966\n",
      "We are now at 375\n",
      "total reward 68.8009010882\n",
      "We are now at 376\n",
      "total reward 75.9275518598\n",
      "We are now at 377\n",
      "total reward 76.2294043604\n",
      "We are now at 378\n",
      "total reward 76.1803499959\n",
      "We are now at 379\n",
      "total reward 79.0679545949\n",
      "saving models\n",
      "average reward 116.274578542\n",
      "We are now at 380\n",
      "total reward 84.6461161228\n",
      "We are now at 381\n",
      "total reward 75.9777321458\n",
      "We are now at 382\n",
      "total reward 70.5076844956\n",
      "We are now at 383\n",
      "total reward 76.5573762079\n",
      "We are now at 384\n",
      "total reward 76.0567653346\n",
      "saving models\n",
      "average reward 116.48841446\n",
      "We are now at 385\n",
      "total reward 85.3647291346\n",
      "We are now at 386\n",
      "total reward 83.5730156557\n",
      "We are now at 387\n",
      "total reward 71.4835600883\n",
      "We are now at 388\n",
      "total reward 76.0917610903\n",
      "We are now at 389\n",
      "total reward 76.4398787871\n",
      "saving models\n",
      "average reward 116.705672419\n",
      "We are now at 390\n",
      "total reward 71.9524305316\n",
      "We are now at 391\n",
      "total reward 76.1238338905\n",
      "We are now at 392\n",
      "total reward 76.0060544878\n",
      "We are now at 393\n",
      "total reward 70.0649884023\n",
      "We are now at 394\n",
      "total reward 78.5296334433\n",
      "saving models\n",
      "average reward 116.721743596\n",
      "We are now at 395\n",
      "total reward 76.1624921595\n",
      "We are now at 396\n",
      "total reward 67.5792894389\n",
      "We are now at 397\n",
      "total reward 79.4066897013\n",
      "We are now at 398\n",
      "total reward 73.5828844825\n",
      "We are now at 399\n",
      "total reward 75.9644101956\n",
      "saving models\n",
      "average reward 116.496833819\n",
      "We are now at 400\n",
      "total reward 67.9550448421\n",
      "We are now at 401\n",
      "total reward 83.9938206657\n",
      "We are now at 402\n",
      "total reward 68.9517640124\n",
      "We are now at 403\n",
      "total reward 76.3837633819\n",
      "We are now at 404\n",
      "total reward 44.2821823604\n",
      "saving models\n",
      "average reward 116.257852418\n",
      "We are now at 405\n",
      "total reward 43.995452343\n",
      "We are now at 406\n",
      "total reward 48.8977706354\n",
      "We are now at 407\n",
      "total reward 48.7199506168\n",
      "We are now at 408\n",
      "total reward 48.879435547\n",
      "We are now at 409\n",
      "total reward 44.2060486051\n",
      "saving models\n",
      "average reward 114.027904195\n",
      "We are now at 410\n",
      "total reward 43.933711144\n",
      "We are now at 411\n",
      "total reward 44.227426459\n",
      "We are now at 412\n",
      "total reward 44.1270206962\n",
      "We are now at 413\n",
      "total reward 43.923163318\n",
      "We are now at 414\n",
      "total reward 44.387418585\n",
      "saving models\n",
      "average reward 112.767747897\n",
      "We are now at 415\n",
      "total reward 43.9497638933\n",
      "We are now at 416\n",
      "total reward 44.3872412594\n",
      "We are now at 417\n",
      "total reward 44.0095440545\n",
      "We are now at 418\n",
      "total reward 44.20689231\n",
      "We are now at 419\n",
      "total reward 48.7963075261\n",
      "saving models\n",
      "average reward 111.745668457\n",
      "We are now at 420\n",
      "total reward 48.8011145822\n",
      "We are now at 421\n",
      "total reward 48.6612660539\n",
      "We are now at 422\n",
      "total reward 48.8503281751\n",
      "We are now at 423\n",
      "total reward 48.8848755999\n",
      "We are now at 424\n",
      "total reward 48.8389059662\n",
      "saving models\n",
      "average reward 111.125428037\n",
      "We are now at 425\n",
      "total reward 48.9611023266\n",
      "We are now at 426\n",
      "total reward 44.0653873914\n",
      "We are now at 427\n",
      "total reward 44.4267263296\n",
      "We are now at 428\n",
      "total reward 49.021859743\n",
      "We are now at 429\n",
      "total reward 44.0629756258\n",
      "saving models\n",
      "average reward 110.558292356\n",
      "We are now at 430\n",
      "total reward 44.1839328851\n",
      "We are now at 431\n",
      "total reward 48.8323307731\n",
      "We are now at 432\n",
      "total reward 44.0428927482\n",
      "We are now at 433\n",
      "total reward 44.0783992276\n",
      "We are now at 434\n",
      "total reward 44.1753790258\n",
      "saving models\n",
      "average reward 110.039243625\n",
      "We are now at 435\n",
      "total reward 48.6889230165\n",
      "We are now at 436\n",
      "total reward 44.0009755243\n",
      "We are now at 437\n",
      "total reward 44.0160455322\n",
      "We are now at 438\n",
      "total reward 48.9323367218\n",
      "We are now at 439\n",
      "total reward 48.9912461118\n",
      "saving models\n",
      "average reward 109.845462536\n",
      "We are now at 440\n",
      "total reward 44.5124348929\n",
      "We are now at 441\n",
      "total reward 44.0052271711\n",
      "We are now at 442\n",
      "total reward 44.0299837457\n",
      "We are now at 443\n",
      "total reward 44.3091633301\n",
      "We are now at 444\n",
      "total reward 48.8437350666\n",
      "saving models\n",
      "average reward 109.616428494\n",
      "We are now at 445\n",
      "total reward 44.3244736688\n",
      "We are now at 446\n",
      "total reward 48.8004769916\n",
      "We are now at 447\n",
      "total reward 43.9934351983\n",
      "We are now at 448\n",
      "total reward 43.9451778944\n",
      "We are now at 449\n",
      "total reward 43.9658926906\n",
      "saving models\n",
      "average reward 109.616659026\n",
      "We are now at 450\n",
      "total reward 44.1179823603\n",
      "We are now at 451\n",
      "total reward 44.0877585141\n",
      "We are now at 452\n",
      "total reward 44.1039828682\n",
      "We are now at 453\n",
      "total reward 49.0096569186\n",
      "We are now at 454\n",
      "total reward 44.2425817752\n",
      "saving models\n",
      "average reward 109.748596663\n",
      "We are now at 455\n",
      "total reward 44.0344562667\n",
      "We are now at 456\n",
      "total reward 44.2403733442\n",
      "We are now at 457\n",
      "total reward 48.9059657775\n",
      "We are now at 458\n",
      "total reward 48.8525011284\n",
      "We are now at 459\n",
      "total reward 44.1175882208\n",
      "saving models\n",
      "average reward 109.940115836\n",
      "We are now at 460\n",
      "total reward 44.0801866639\n",
      "We are now at 461\n",
      "total reward 48.9599742256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are now at 462\n",
      "total reward 49.0491466998\n",
      "We are now at 463\n",
      "total reward 43.8328352883\n",
      "We are now at 464\n",
      "total reward 44.1441438693\n",
      "saving models\n",
      "average reward 109.73650909\n",
      "We are now at 465\n",
      "total reward 43.9488871448\n",
      "We are now at 466\n",
      "total reward 44.1378384484\n",
      "We are now at 467\n",
      "total reward 44.1945699237\n",
      "We are now at 468\n",
      "total reward 44.2087760033\n",
      "We are now at 469\n",
      "total reward 44.0157993526\n",
      "saving models\n",
      "average reward 109.540916474\n",
      "We are now at 470\n",
      "total reward 44.3659003797\n",
      "We are now at 471\n",
      "total reward 48.7072342255\n",
      "We are now at 472\n",
      "total reward 44.1225578245\n",
      "We are now at 473\n",
      "total reward 49.0290866504\n",
      "We are now at 474\n",
      "total reward 43.9325738138\n",
      "saving models\n",
      "average reward 109.544282177\n",
      "We are now at 475\n",
      "total reward 48.8953045469\n",
      "We are now at 476\n",
      "total reward 44.3097957278\n",
      "We are now at 477\n",
      "total reward 43.9814990278\n",
      "We are now at 478\n",
      "total reward 44.6909370678\n",
      "We are now at 479\n",
      "total reward 44.1689444034\n",
      "saving models\n",
      "average reward 109.588285938\n",
      "We are now at 480\n",
      "total reward 44.2747297298\n",
      "We are now at 481\n",
      "total reward 44.1101112719\n",
      "We are now at 482\n",
      "total reward 44.1316366326\n",
      "We are now at 483\n",
      "total reward 44.0070611739\n",
      "We are now at 484\n",
      "total reward 44.0667116055\n",
      "saving models\n",
      "average reward 109.623922264\n",
      "We are now at 485\n",
      "total reward 44.1873894524\n",
      "We are now at 486\n",
      "total reward 44.2424437153\n",
      "We are now at 487\n",
      "total reward 48.744365874\n",
      "We are now at 488\n",
      "total reward 44.2445871093\n",
      "We are now at 489\n",
      "total reward 44.0420747284\n",
      "saving models\n",
      "average reward 109.618541178\n",
      "We are now at 490\n",
      "total reward 48.7090901399\n",
      "We are now at 491\n",
      "total reward 44.1949233268\n",
      "We are now at 492\n",
      "total reward 44.3825532896\n",
      "We are now at 493\n",
      "total reward 44.1266523139\n",
      "We are now at 494\n",
      "total reward 44.0569027499\n",
      "saving models\n",
      "average reward 109.660282975\n",
      "We are now at 495\n",
      "total reward 48.5591275402\n",
      "We are now at 496\n",
      "total reward 48.9220067755\n",
      "We are now at 497\n",
      "total reward 48.6796722856\n",
      "We are now at 498\n",
      "total reward 44.1234180032\n",
      "We are now at 499\n",
      "total reward 48.9046317337\n",
      "saving models\n",
      "average reward 109.490679094\n",
      "We are now at 500\n",
      "total reward 44.1289858902\n",
      "We are now at 501\n",
      "total reward 44.0861978959\n",
      "We are now at 502\n",
      "total reward 43.9113611723\n",
      "We are now at 503\n",
      "total reward 44.0002722225\n",
      "We are now at 504\n",
      "total reward 49.0035890372\n",
      "saving models\n",
      "average reward 109.53670158\n",
      "We are now at 505\n",
      "total reward 48.754607538\n",
      "We are now at 506\n",
      "total reward 43.9915108702\n"
     ]
    }
   ],
   "source": [
    "#Main loop\n",
    "S = env.reset()\n",
    "#action_count=0\n",
    "episode_count=0\n",
    "save= True\n",
    "total_reward =0 \n",
    "total_reward_list=[]\n",
    "while(True):\n",
    "    \n",
    "    #renders environment \n",
    "    env.render()\n",
    "    \n",
    "    \n",
    "    #Selects action according to stochastic policy\n",
    "    action = select_action(S)\n",
    "    #action_count+=1\n",
    "    \n",
    "    #Takes action \n",
    "    S1, reward, done, info = env.step(action)\n",
    "    \n",
    "    \n",
    "    #Updates weights\n",
    "    #if(np.random.randint(10)%5==0):\n",
    "    update_weights(S , action, reward, S1 , done)\n",
    "    \n",
    "    S = S1\n",
    "    total_reward +=reward\n",
    "    \n",
    "    if(done):\n",
    "        \n",
    "        print('We are now at '+str(episode_count))\n",
    "        gc.collect()\n",
    "        print('total reward '+ str(total_reward))\n",
    "        total_reward_list.append(total_reward)\n",
    "        total_reward=0\n",
    "        \n",
    "        #Resets episode\n",
    "        S = env.reset()\n",
    "        #action_count=0\n",
    "        episode_count+=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(episode_count%5==0):\n",
    "            \n",
    "            if(save):\n",
    "                print('saving models')\n",
    "                print('average reward '+ str(average_reward))\n",
    "                \n",
    "                preference_model.save('preference.h5')\n",
    "                policy_model.save('policy.h5')\n",
    "                value_model.save('value.h5')\n",
    "                \n",
    "                pickle.dump(rewardz_received, open('rewardz_received.ferch','wb'))\n",
    "                pickle.dump(delta_rewardz_received, open('delta_rewardz_received.ferch','wb'))\n",
    "                pickle.dump(average_reward, open('average_reward.ferch','wb'))\n",
    "                pickle.dump(total_reward_list, open('total_rewards_hist','wb'))\n",
    "                \n",
    "                #Restarting keras session \n",
    "                K.clear_session()\n",
    "                sess = K.get_session()\n",
    "                preference_model = load_model('preference.h5')\n",
    "                policy_model = load_model('policy.h5')\n",
    "                value_model = load_model('value.h5')\n",
    "\n",
    "        \n",
    "        #Resets eligibility traces\n",
    "        global value_eligibility_traces, policy_eligibility_traces\n",
    "        policy_eligibility_traces= [tf.zeros(shape = tensor.eval(session = sess).shape) for tensor in policy_model.trainable_weights]\n",
    "        value_eligibility_traces= [tf.zeros(shape = tensor.eval(session = sess).shape) for tensor in value_model.trainable_weights]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
