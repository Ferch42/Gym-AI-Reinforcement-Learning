{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "#from keras.constraints import MaxNorm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the use of GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/gpu:0']\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "sess= K.get_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Humanoid-v2')\n",
    "So = env.reset()\n",
    "A = env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting learning hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Policy\n",
    "policy_alpha = 0.01\n",
    "policy_lambda =  0.9\n",
    "#Value\n",
    "value_alpha = 0.01\n",
    "value_lambda = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model for policy mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Mean network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_1 (Dropout)          (None, 376)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                11310     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 17)                527       \n",
      "=================================================================\n",
      "Total params: 13,697\n",
      "Trainable params: 13,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "policy_model_mean = Sequential()\n",
    "policy_model_mean.add(Dropout(0.1,input_shape=So.shape))\n",
    "policy_model_mean.add(Dense(30 ,kernel_initializer='random_uniform', activation = 'relu' ))\n",
    "policy_model_mean.add(Dropout(0.5))\n",
    "policy_model_mean.add(Dense(30,kernel_initializer='random_uniform', activation = 'relu'))\n",
    "policy_model_mean.add(Dropout(0.5))\n",
    "policy_model_mean.add(Dense(30,kernel_initializer='random_uniform', activation = 'relu'))\n",
    "policy_model_mean.add(Dropout(0.5))\n",
    "policy_model_mean.add(Dense( A.shape[0],kernel_initializer='random_uniform', activation='linear' ))\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "policy_model_mean.compile(loss=\"mse\", optimizer= adam)\n",
    "print(\"Policy Mean network\")\n",
    "print(policy_model_mean.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model for policy std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy std network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_5 (Dropout)          (None, 376)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 30)                11310     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 17)                527       \n",
      "=================================================================\n",
      "Total params: 13,697\n",
      "Trainable params: 13,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "policy_model_std = Sequential()\n",
    "policy_model_std.add(Dropout(0.1,input_shape=So.shape))\n",
    "policy_model_std.add(Dense(30 ,kernel_initializer='random_uniform', activation = 'relu' ))\n",
    "policy_model_std.add(Dropout(0.5))\n",
    "policy_model_std.add(Dense(30,kernel_initializer='random_uniform', activation = 'relu'))\n",
    "policy_model_std.add(Dropout(0.5))\n",
    "policy_model_std.add(Dense(30,kernel_initializer='random_uniform', activation = 'relu'))\n",
    "policy_model_std.add(Dropout(0.5))\n",
    "policy_model_std.add(Dense( A.shape[0],kernel_initializer='random_uniform', activation='linear' ))\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "policy_model_std.compile(loss=\"mse\", optimizer= adam)\n",
    "print(\"Policy std network\")\n",
    "print(policy_model_std.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating model for values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_9 (Dropout)          (None, 376)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 30)                11310     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 13,201\n",
      "Trainable params: 13,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "value_model = Sequential()\n",
    "value_model.add(Dropout(0.1,input_shape=So.shape))\n",
    "value_model.add(Dense(30, kernel_initializer='random_uniform',activation = 'relu' ))\n",
    "value_model.add(Dropout(0.5))\n",
    "value_model.add(Dense(30, kernel_initializer='random_uniform',activation = 'relu'))\n",
    "value_model.add(Dropout(0.5))\n",
    "value_model.add(Dense(30, kernel_initializer='random_uniform',activation = 'relu'))\n",
    "value_model.add(Dropout(0.5))\n",
    "value_model.add(Dense(1,kernel_initializer='random_uniform', activation= 'linear'))\n",
    "\n",
    "print(\"Value Model\")\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "value_model.compile(loss=\"mse\", optimizer= adam)\n",
    "print(value_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eligibility Traces for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "policy_eligibility_traces_mean= [tf.Variable(np.zeros(shape = sess.run(tf.shape(tensor))), dtype= tf.float32) for tensor in policy_model_mean.trainable_weights]\n",
    "policy_eligibility_traces_std= [tf.Variable(np.zeros(shape = sess.run(tf.shape(tensor))), dtype= tf.float32) for tensor in policy_model_std.trainable_weights]\n",
    "value_eligibility_traces= [tf.Variable(np.zeros(shape = sess.run(tf.shape(tensor))), dtype= tf.float32) for tensor in value_model.trainable_weights]\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining other hyperparameters concerning the reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_reward = 5 #pickle.load(open('average_reward.ferch','rb'))\n",
    "cumulative_reward=0\n",
    "etaa = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(True):\n",
    "    policy_model_mean = load_model('policy_mean.h5')\n",
    "    policy_model_std = load_model('policy_std.h5')\n",
    "    value_model = load_model('value.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions for weights update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_weights(model, weights):  \n",
    "    if(model=='value'):\n",
    "        i=0\n",
    "        for layer in value_model.layers:\n",
    "            if 'dropout' not in layer.name:\n",
    "                layer.set_weights([weights[i],weights[i+1]])\n",
    "                i+=2\n",
    "    elif(model=='policy_mean'):\n",
    "        i=0\n",
    "        for layerz in policy_model_mean.layers:\n",
    "            if 'dropout' not in layerz.name:\n",
    "                layerz.set_weights([weights[i],weights[i+1]])\n",
    "                i+=2\n",
    "                \n",
    "    elif(model=='policy_std'):\n",
    "        i=0\n",
    "        for layerz in policy_model_std.layers:\n",
    "            if 'dropout' not in layerz.name:\n",
    "                layerz.set_weights([weights[i],weights[i+1]])\n",
    "                i+=2\n",
    "    else:\n",
    "        print('error')\n",
    "\n",
    "def get_value_weights():\n",
    "    weights =[]\n",
    "    for w in value_model.layers:\n",
    "        weights+= w.weights\n",
    "    return weights\n",
    "\n",
    "def get_policy_weights_mean():\n",
    "    weights =[]\n",
    "    for w in policy_model_mean.layers:\n",
    "        weights+= w.weights\n",
    "    return weights\n",
    "\n",
    "def get_policy_weights_std():\n",
    "    weights =[]\n",
    "    for w in policy_model_std.layers:\n",
    "        weights+= w.weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_action(action):\n",
    "    #FIXING ACTION INCONSISTENCIES\n",
    "    low = env.action_space.low\n",
    "    high = env.action_space.high\n",
    "    \n",
    "    for i in range(action.shape[0]):\n",
    "        if(action[i]>high[i]):\n",
    "            return True\n",
    "        if(action[i]<low[i]):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_weights = get_value_weights()\n",
    "policy_weights_mean = get_policy_weights_mean()\n",
    "policy_weights_std = get_policy_weights_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(previous_state, action , reward, state,  terminal, skip_count):\n",
    "    \n",
    "    #Compute rewards\n",
    "    global average_reward,rewardz_received,delta_rewardz_received, cumulative_reward \n",
    "    delta_rewards = reward - average_reward*skip_count + value_model.predict(np.array([state]))[0][0] - value_model.predict(np.array([previous_state]))[0][0]    \n",
    "    cumulative_reward = (cumulative_reward + reward)/2\n",
    "    print(value_model.predict(np.array([previous_state]))[0][0] ,\"=====\",value_model.predict(np.array([state]))[0][0],\"====\",delta_rewards)\n",
    "    if(terminal):\n",
    "        delta_rewards = reward - average_reward*skip_count + 0  - value_model.predict(np.array([previous_state]))[0][0]\n",
    "    thresh = average_reward + cumulative_reward\n",
    "    if(delta_rewards>=thresh):\n",
    "        delta_rewards =thresh\n",
    "    if(delta_rewards<=-thresh):\n",
    "        delta_rewards=-thresh\n",
    "    average_reward = average_reward + etaa*delta_rewards\n",
    "    \n",
    "    #Compute value updates (eligibility traces and weights)\n",
    "    global value_eligibility_traces , value_weights , policy_weights_std\n",
    "    value_gradients = value_model.optimizer.get_gradients(value_model.output, value_model.trainable_weights)\n",
    "    value_gradients = [tf.clip_by_norm(gradient, 1) for gradient in value_gradients]\n",
    "    value_eligibility_traces_op = [value_eligibility_traces[i].assign(tf.add(tf.multiply(value_eligibility_traces[i], tf.constant(value_lambda, dtype = tf.float32)),value_gradients[i] )) for i in range(len(value_eligibility_traces))] \n",
    "    sess.run(value_eligibility_traces_op,feed_dict={value_model.input:np.array([previous_state])})    \n",
    "    value_weights_op =  [value_weights[i].assign(tf.add(value_weights[i], tf.multiply(tf.constant(value_alpha*delta_rewards, dtype= tf.float32), value_eligibility_traces[i]))) for i in range(len(value_weights))]\n",
    "    sess.run(value_weights_op)\n",
    "    \n",
    "    #Compute policy updates (eligibility traces and weights)\n",
    "    \n",
    "    global policy_eligibility_traces_mean, policy_eligibility_traces_std , policy_weights_mean\n",
    "    policy_mean = policy_model_mean.predict(np.array([previous_state]))[0]\n",
    "    policy_std = policy_model_std.predict(np.array([previous_state]))[0]\n",
    "    policy_std= policy_std*policy_std\n",
    "    \n",
    "    ##################################################  MEAN ##################################################################\n",
    "    loss_mean = tf.divide(tf.subtract(tf.constant(action, dtype = tf.float32), policy_model_mean.output), tf.constant(policy_std*policy_std, dtype = tf.float32))\n",
    "    policy_gradients_mean = policy_model_mean.optimizer.get_gradients(loss_mean, policy_model_mean.trainable_weights)\n",
    "    policy_gradients_mean = [tf.clip_by_norm(gradient, 1) for gradient in policy_gradients_mean]\n",
    "\n",
    "    policy_eligibility_traces_op_mean = [policy_eligibility_traces_mean[i].assign(tf.add(tf.multiply(tf.constant(policy_lambda, dtype = tf.float32), policy_eligibility_traces_mean[i]), policy_gradients_mean[i])) for i in range(len(policy_gradients_mean))]\n",
    "    sess.run(policy_eligibility_traces_op_mean, feed_dict={policy_model_mean.input:np.array([previous_state])})\n",
    "    policy_weights_op_mean =  [policy_weights_mean[i].assign(tf.add(policy_weights_mean[i], tf.multiply(policy_eligibility_traces_mean[i],tf.constant(policy_alpha*delta_rewards, dtype = tf.float32)))) for i in range(len(policy_weights_mean))]\n",
    "    sess.run(policy_weights_op_mean)    \n",
    "    \n",
    "    ##################################################   STD ###################################################################\n",
    "    loss_std = 2* tf.subtract(tf.divide(tf.square(tf.subtract(tf.constant(action, dtype = tf.float32),policy_mean)), tf.pow(policy_model_std.output, 3)), tf.divide(1, policy_model_std.output))\n",
    "    policy_gradients_std = policy_model_std.optimizer.get_gradients(loss_std, policy_model_std.trainable_weights)\n",
    "    policy_gradients_std = [tf.clip_by_norm(gradient, 1) for gradient in policy_gradients_std]\n",
    "\n",
    "    policy_eligibility_traces_op_std = [policy_eligibility_traces_std[i].assign(tf.add(tf.multiply(tf.constant(policy_lambda, dtype = tf.float32), policy_eligibility_traces_std[i]), policy_gradients_std[i])) for i in range(len(policy_gradients_std))]\n",
    "    sess.run(policy_eligibility_traces_op_std, feed_dict={policy_model_std.input:np.array([previous_state])})\n",
    "    policy_weights_op_std =  [policy_weights_std[i].assign(tf.add(policy_weights_std[i], tf.multiply( policy_eligibility_traces_std[i],tf.constant(policy_alpha*delta_rewards, dtype = tf.float32)))) for i in range(len(policy_weights_std))]\n",
    "    sess.run(policy_weights_op_std)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    \n",
    "    mean = policy_model_mean.predict(np.array([state]))[0]\n",
    "    std = policy_model_std.predict(np.array([state]))[0]\n",
    "    std= std*std\n",
    "    #print(mean)\n",
    "    covariance = np.zeros([mean.shape[0],mean.shape[0]])\n",
    "    \n",
    "    for i in range(std.shape[0]):\n",
    "        covariance[i][i]= std[i]\n",
    "        \n",
    "    action = np.random.multivariate_normal(mean, covariance)\n",
    "    #print(action)\n",
    "\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'      \\n    #FIXING ACTION INCONSISTENCIES\\n    low = env.action_space.low\\n    high = env.action_space.high\\n    \\n    for i in range(action.shape[0]):\\n        if(action[i]>high[i]):\\n            action[i]= high[i]+0.1\\n        if(action[i]<low[i]):\\n            action[i]=low[i]+0.1\\n            \\\\    high = env.action_space.high\\n    global average_reward_variance\\n    print(average_reward_variance)\\n    if(np.abs(average_reward_variance)<0.1):\\n        print(\"Sampling randomly\")\\n        return env.action_space.sample()\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''      \n",
    "    #FIXING ACTION INCONSISTENCIES\n",
    "    low = env.action_space.low\n",
    "    high = env.action_space.high\n",
    "    \n",
    "    for i in range(action.shape[0]):\n",
    "        if(action[i]>high[i]):\n",
    "            action[i]= high[i]+0.1\n",
    "        if(action[i]<low[i]):\n",
    "            action[i]=low[i]+0.1\n",
    "            \\    high = env.action_space.high\n",
    "    global average_reward_variance\n",
    "    print(average_reward_variance)\n",
    "    if(np.abs(average_reward_variance)<0.1):\n",
    "        print(\"Sampling randomly\")\n",
    "        return env.action_space.sample()\n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Episodes\n",
    "## Setting up variables :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S = env.reset()\n",
    "#action_count=0\n",
    "episode_count=0\n",
    "save= True\n",
    "total_reward =0 \n",
    "total_reward_list=[]\n",
    "skip_count = 0\n",
    "skip_reward=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.141645 ===== 0.146498 ==== 0.0112097904481\n",
      "0.146615 ===== 0.140383 ==== -0.00147634406962\n",
      "0.140354 ===== 0.141884 ==== 0.0111572555472\n",
      "0.1422 ===== 0.152121 ==== 0.00886367043355\n",
      "0.152441 ===== 0.144999 ==== 0.000646924287369\n",
      "0.145026 ===== 0.144466 ==== 0.00453353249318\n",
      "0.144689 ===== 0.145164 ==== 0.00451126343992\n",
      "0.145411 ===== 0.14282 ==== 0.00574620815404\n",
      "0.143161 ===== 0.149676 ==== 0.00582055281112\n",
      "0.150055 ===== 1.66802 ==== -0.475511735568\n",
      "1.60048 ===== 1.65026 ==== -0.82161593423\n",
      "1.3203 ===== 0.5197 ==== -0.961756823936\n",
      "0.276677 ===== 0.199198 ==== -0.231446451188\n",
      "0.147224 ===== 0.118266 ==== -0.171987295258\n",
      "0.0794302 ===== 0.00506609 ==== -0.161575880207\n",
      "-0.0193725 ===== 0.0026481 ==== -0.0969589490909\n",
      "-0.0144996 ===== -0.015064 ==== -0.105538501336\n",
      "-0.0337295 ===== -0.0287898 ==== -0.114992282381\n",
      "-0.0488598 ===== -0.0586602 ==== -0.137735158473\n",
      "-0.0790989 ===== -0.0769805 ==== -0.120360201908\n",
      "-0.0943724 ===== -0.0659443 ==== -0.150154783329\n",
      "-0.0884705 ===== -0.098183 ==== -0.208034752853\n",
      "-0.125105 ===== -0.131992 ==== -0.179476411402\n",
      "-0.153464 ===== -0.151322 ==== -0.182753775654\n",
      "-0.172611 ===== -0.171537 ==== -0.201107971686\n",
      "-0.194323 ===== -0.196836 ==== -0.214095204662\n",
      "-0.219203 ===== -0.217151 ==== -0.197487557239\n",
      "-0.237597 ===== -0.237373 ==== -0.1931804677\n",
      "-0.256794 ===== -0.260752 ==== -0.233891918087\n",
      "-0.285176 ===== -0.28019 ==== -0.241612006419\n",
      "-0.306074 ===== -0.308516 ==== -0.228213826397\n",
      "-0.333399 ===== -0.331919 ==== -0.223895036323\n",
      "-0.355841 ===== -0.356961 ==== -0.189362053317\n",
      "-0.377515 ===== -0.384536 ==== -0.216744498086\n",
      "-0.409543 ===== -0.406043 ==== -0.179456331219\n",
      "-0.42699 ===== -0.442071 ==== -0.284432917032\n",
      "0 --total reward =171.712560118\n",
      "0 --average reward =4.93769705268\n",
      "saving models\n",
      "average reward 4.93769705268\n",
      "0.000381938 ===== 0.000308655 ==== 0.0573679875031\n",
      "0.000887996 ===== 0.000905709 ==== 0.056885206492\n",
      "0.00199739 ===== 0.0020239 ==== 0.0563250924081\n",
      "0.00356579 ===== 0.00358594 ==== 0.0557535109303\n",
      "0.0055229 ===== 0.00554325 ==== 0.0551915852267\n",
      "0.00782649 ===== 0.007849 ==== 0.0546443312662\n",
      "0.010435 ===== 0.0104535 ==== 0.054091373935\n",
      "0.0133039 ===== 0.0133093 ==== 0.0535338576364\n",
      "0.0163891 ===== 0.01641 ==== 0.0529957611878\n",
      "0.0196894 ===== 0.0196253 ==== 0.0527261914125\n",
      "0.0230929 ===== 0.0633497 ==== -1.1059408373\n",
      "-0.0153795 ===== -0.0385883 ==== -0.108640612082\n",
      "-0.0496608 ===== -0.0569293 ==== 0.0826720125646\n",
      "-0.0489098 ===== -0.0517215 ==== 0.111806576872\n",
      "-0.0412515 ===== -0.0412837 ==== 0.116276928573\n",
      "-0.0304506 ===== -0.0299122 ==== 0.113908350094\n",
      "-0.0187893 ===== -0.018252 ==== 0.115799679812\n",
      "-0.00650891 ===== -0.00631975 ==== 0.120776181536\n",
      "0.00612173 ===== 0.00608227 ==== 0.131176169217\n",
      "0.0196362 ===== 0.0194898 ==== 0.141683747523\n",
      "0.0341527 ===== 0.0344245 ==== 0.151845705914\n",
      "0.0506696 ===== 0.0496574 ==== 0.165721986849\n",
      "0.067839 ===== 0.0669557 ==== 0.179658887316\n",
      "0.0872642 ===== 0.0855495 ==== 0.195361400293\n",
      "0.108237 ===== 0.105609 ==== 0.210940949365\n",
      "0.130819 ===== 0.12914 ==== 0.226782145977\n",
      "0.157266 ===== 0.143815 ==== 0.234668474496\n",
      "0.171915 ===== 0.185644 ==== 0.274823522785\n",
      "0.222969 ===== 0.223289 ==== 0.276432640662\n",
      "0.263629 ===== 0.218565 ==== 0.253631778046\n",
      "0.247846 ===== 0.279447 ==== 0.332166680334\n",
      "0.32918 ===== 0.317261 ==== 0.298752807162\n",
      "0.363856 ===== 0.293805 ==== 0.2566351364\n",
      "0.321078 ===== 0.401988 ==== 0.402208849272\n",
      "0.471544 ===== 0.408187 ==== 0.258917676936\n",
      "0.445836 ===== 0.533539 ==== 0.402603468219\n",
      "0.625468 ===== 0.676832 ==== 0.372931376625\n",
      "0.791886 ===== 0.555537 ==== 0.107225078746\n",
      "1 --total reward =193.076773922\n",
      "1 --average reward =4.98084509943\n",
      "saving models\n",
      "average reward 4.98084509943\n",
      "0.000492888 ===== 0.000305468 ==== 0.0199227139071\n",
      "0.000509979 ===== 0.000419292 ==== 0.0198202195321\n",
      "0.000804869 ===== 0.000754439 ==== 0.019662306843\n",
      "0.00130003 ===== 0.00126215 ==== 0.0194782227362\n",
      "0.00194803 ===== 0.00184084 ==== 0.019213672735\n",
      "0.00264625 ===== 0.0026087 ==== 0.0190914818432\n",
      "0.00352251 ===== 0.00343968 ==== 0.0188551556137\n",
      "0.0044444 ===== 0.00434572 ==== 0.018650274727\n",
      "0.00542821 ===== 0.00543996 ==== 0.0185702113681\n",
      "0.00659932 ===== 0.00659019 ==== 0.0183672111491\n",
      "0.00780821 ===== 0.00576145 ==== -2.18459526853\n",
      "-0.161065 ===== -0.152636 ==== -0.321291144305\n",
      "-0.195373 ===== -0.179557 ==== 0.00395505622982\n",
      "-0.179064 ===== -0.176226 ==== 0.0154236359206\n",
      "-0.174385 ===== -0.170914 ==== 0.0119097993238\n",
      "-0.169532 ===== -0.170249 ==== 0.0047562973044\n",
      "-0.169666 ===== -0.168315 ==== 0.00394148321444\n",
      "-0.167802 ===== -0.169098 ==== 0.00118443441212\n",
      "-0.168934 ===== -0.168992 ==== 0.00575462251311\n",
      "-0.168142 ===== -0.170264 ==== 0.00218114209116\n",
      "-0.169946 ===== -0.169717 ==== 0.0151247058282\n",
      "-0.167422 ===== -0.166408 ==== 0.0233561435548\n",
      "-0.162544 ===== -0.163132 ==== 0.0255781591177\n",
      "-0.158802 ===== -0.158757 ==== 0.0299317630589\n",
      "-0.153639 ===== -0.153716 ==== 0.0375713818592\n",
      "-0.147202 ===== -0.146502 ==== 0.038957901494\n",
      "-0.13931 ===== -0.140702 ==== 0.0377052051963\n",
      "-0.134085 ===== -0.133158 ==== 0.0500782773566\n",
      "-0.124536 ===== -0.125859 ==== 0.052230793836\n",
      "-0.116422 ===== -0.11598 ==== 0.0599546091348\n",
      "-0.105723 ===== -0.107072 ==== 0.0507065131407\n",
      "-0.0979408 ===== -0.0970524 ==== 0.0578021706766\n",
      "-0.0869191 ===== -0.0872294 ==== 0.0666060183916\n",
      "-0.0773725 ===== -0.0751538 ==== 0.0751896883526\n",
      "-0.0630819 ===== -0.063522 ==== 0.0789727566185\n",
      "-0.0504322 ===== -0.0528604 ==== 0.078953622578\n",
      "-0.0390535 ===== -0.0421134 ==== 0.0888751821689\n",
      "-0.0266769 ===== -0.02951 ==== 0.108615509376\n",
      "-0.0101045 ===== -0.0063299 ==== 0.126460626532\n",
      "0.0186786 ===== 0.00877737 ==== 0.122308328694\n",
      "0.0342981 ===== 0.0296469 ==== 0.14526671949\n",
      "0.0618752 ===== 0.116032 ==== 0.179828924172\n",
      "0.173289 ===== 0.139534 ==== 0.0854500334859\n",
      "0.169178 ===== 0.176903 ==== 0.137259596873\n",
      "0.231828 ===== 0.198391 ==== 0.0985684189549\n",
      "0.239989 ===== 0.263424 ==== 0.154855355743\n",
      "0.345428 ===== 0.320741 ==== 0.117424149555\n",
      "0.389929 ===== 0.761567 ==== 0.353320130594\n",
      "1.28271 ===== 0.823334 ==== -0.330916551827\n",
      "0.498 ===== 0.444551 ==== 0.0976716072754\n",
      "0.524797 ===== 0.59677 ==== 0.221459588093\n",
      "0.821796 ===== 0.70603 ==== 0.0807279095022\n",
      "0.794298 ===== 0.652446 ==== 0.100269287774\n",
      "0.748765 ===== 0.784751 ==== 0.284666871493\n",
      "1.14729 ===== 0.967858 ==== 0.0919503781412\n",
      "1.08628 ===== 1.03559 ==== 0.231122655032\n",
      "1.37105 ===== 1.23259 ==== 0.151710564813\n",
      "1.46705 ===== 1.61255 ==== 0.437885789488\n",
      "2.591 ===== 2.13988 ==== -0.15183189568\n",
      "1.82603 ===== 1.94023 ==== 0.425751303686\n",
      "3.00518 ===== 1.71578 ==== -0.973702778651\n",
      "0.60545 ===== 0.904639 ==== 0.627119105816\n",
      "1.88097 ===== 1.52377 ==== -0.0228497030996\n",
      "1.48727 ===== 1.40742 ==== 0.259494335331\n",
      "1.8404 ===== 2.24785 ==== 0.744653319302\n",
      "4.52304 ===== 0.710317 ==== -3.46948635053\n",
      "-0.036117 ===== -0.0358808 ==== 0.380048548406\n",
      "0.00553928 ===== 0.00644485 ==== 0.377313740932\n",
      "0.0512619 ===== 0.045387 ==== 0.367608763781\n",
      "2 --total reward =349.08012524\n",
      "2 --average reward =4.98201593669\n",
      "saving models\n",
      "average reward 4.98201593669\n",
      "7.23495e-06 ===== -4.47871e-05 ==== 0.0128772150272\n",
      "8.49474e-05 ===== 7.99615e-05 ==== 0.0127954537612\n",
      "0.000324936 ===== 0.000331341 ==== 0.0126788586096\n",
      "0.000677602 ===== 0.000688559 ==== 0.0125566953424\n",
      "0.0011239 ===== 0.00113099 ==== 0.0124271178668\n",
      "0.00164432 ===== 0.00161952 ==== 0.0122710638916\n",
      "0.00220002 ===== 0.00219055 ==== 0.0121634631602\n",
      "0.00283163 ===== 0.00281104 ==== 0.0120307822844\n",
      "0.00350359 ===== 0.00351212 ==== 0.0119384798199\n",
      "0.00425149 ===== 0.00421451 ==== 0.0117741458222\n",
      "0.00498995 ===== 0.0334237 ==== -2.12895682265\n",
      "-0.117475 ===== -0.128643 ==== -0.327241020269\n",
      "-0.160652 ===== -0.164329 ==== -0.00971482289868\n",
      "-0.165183 ===== -0.167617 ==== 0.035997182662\n",
      "-0.164537 ===== -0.159873 ==== 0.0492877776774\n",
      "-0.155494 ===== -0.156774 ==== 0.0470223236388\n",
      "-0.152511 ===== -0.154394 ==== 0.0407806690555\n",
      "-0.150616 ===== -0.148752 ==== 0.0484507645384\n",
      "-0.144188 ===== -0.144028 ==== 0.0577336182419\n",
      "-0.138363 ===== -0.138353 ==== 0.0625966029815\n",
      "-0.132033 ===== -0.13228 ==== 0.0584428742688\n",
      "-0.126349 ===== -0.124559 ==== 0.0691898101521\n",
      "-0.11732 ===== -0.118405 ==== 0.0738041308629\n",
      "-0.110595 ===== -0.111208 ==== 0.0811483008483\n",
      "-0.10245 ===== -0.103387 ==== 0.0894845422258\n",
      "-0.0936369 ===== -0.094694 ==== 0.0939175499009\n",
      "-0.0844655 ===== -0.0809254 ==== 0.0955535177612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0704034 ===== -0.0720911 ==== 0.0850020003152\n",
      "-0.0625489 ===== -0.0616665 ==== 0.0844009865386\n",
      "-0.0521208 ===== -0.053974 ==== 0.0904591091626\n",
      "-0.0437251 ===== -0.0427704 ==== 0.0879401857659\n",
      "-0.0326488 ===== -0.0315947 ==== 0.0942251887712\n",
      "-0.0208148 ===== -0.0235275 ==== 0.101896894477\n",
      "-0.0117259 ===== -0.00916659 ==== 0.111687976113\n",
      "0.00432723 ===== 0.00740679 ==== 0.112426548196\n",
      "0.0216461 ===== 0.0193348 ==== 0.109415024741\n",
      "0.0334162 ===== 0.0362371 ==== 0.113702218649\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    \n",
    "    #renders environment \n",
    "    #env.render()\n",
    "    \n",
    "    high = env.action_space.high\n",
    "    #Selects action according to stochastic policy\n",
    "    action = select_action(S%high[0])\n",
    "    #action_count+=1\n",
    "    \n",
    "    \n",
    "    #Takes action \n",
    "    S1, reward, done, info = env.step(action)\n",
    "    skip_reward += reward\n",
    "    skip_count += 1\n",
    "    \n",
    "    #Updates weights\n",
    "    #if(np.random.randint(10)%5==0):\n",
    "    if(True):\n",
    "        update_weights(S , action, skip_reward, S1 , done, skip_count)\n",
    "        skip_reward = 0\n",
    "        skip_count =0\n",
    "    \n",
    "    S = S1\n",
    "    total_reward +=reward\n",
    "    \n",
    "    if(done):\n",
    "        \n",
    "        #print('We are now at '+str(episode_count))\n",
    "        gc.collect()\n",
    "        print(str(episode_count)+' --total reward ='+ str(total_reward))\n",
    "        print(str(episode_count)+' --average reward ='+ str(average_reward))\n",
    "        total_reward_list.append(total_reward)\n",
    "        total_reward=0\n",
    "        \n",
    "        #Resets episode\n",
    "        S = env.reset()\n",
    "        #action_count=0\n",
    "        episode_count+=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(episode_count%1==0):\n",
    "            \n",
    "            if(save):\n",
    "                print('saving models')\n",
    "                print('average reward '+ str(average_reward))\n",
    "                \n",
    "                policy_model_mean.save('policy_mean.h5')\n",
    "                policy_model_std.save('policy_std.h5')\n",
    "                value_model.save('value.h5')\n",
    "                pickle.dump(total_reward_list,open('totalz_hist','wb')) \n",
    "                \n",
    "                \n",
    "                #Restarting keras session \n",
    "                K.clear_session()\n",
    "                sess = K.get_session()\n",
    "                policy_model_mean = load_model('policy_mean.h5')\n",
    "                policy_model_std = load_model('policy_std.h5')\n",
    "                value_model = load_model('value.h5')\n",
    "\n",
    "\n",
    "        \n",
    "        #Resets eligibility traces\n",
    "        global value_eligibility_traces, policy_eligibility_traces_mean, policy_eligibility_traces_std\n",
    "        \n",
    "        policy_eligibility_traces_mean= [tf.Variable(np.zeros(shape = sess.run(tf.shape(tensor))), dtype= tf.float32) for tensor in policy_model_mean.trainable_weights]\n",
    "        policy_eligibility_traces_std= [tf.Variable(np.zeros(shape = sess.run(tf.shape(tensor))), dtype= tf.float32) for tensor in policy_model_std.trainable_weights]\n",
    "        value_eligibility_traces= [tf.Variable(np.zeros(shape = sess.run(tf.shape(tensor))), dtype= tf.float32) for tensor in value_model.trainable_weights]\n",
    "\n",
    "        value_weights = get_value_weights()\n",
    "        policy_weights_mean = get_policy_weights_mean()\n",
    "        policy_weights_std = get_policy_weights_std()\n",
    "        \n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
