{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout \n",
    "from keras.optimizers import Adam\n",
    "#from keras.constraints import MaxNorm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Board auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    #Attach a lot of summaries to a Tensor (for TensorBoard visualization).\n",
    "    for i in range(len(var)):\n",
    "        with tf.name_scope(\"Layer_\"+str(i)):\n",
    "            with tf.name_scope('summaries'):\n",
    "                mean = tf.reduce_mean(var[i])\n",
    "                tf.summary.scalar('mean', mean)\n",
    "                with tf.name_scope('stddev'):\n",
    "                    stddev = tf.sqrt(tf.reduce_mean(tf.square(var[i] - mean)))\n",
    "                tf.summary.scalar('stddev', stddev)\n",
    "                tf.summary.scalar('max', tf.reduce_max(var[i]))\n",
    "                tf.summary.scalar('min', tf.reduce_min(var[i]))\n",
    "                tf.summary.histogram('histogram', var[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries2(var):\n",
    "    #Attach a lot of summaries to a Tensor (for TensorBoard visualization).\n",
    "    \n",
    "    with tf.name_scope('summaries'):\n",
    "        tf.summary.scalar('value', var)\n",
    "        tf.summary.histogram('histogram', var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the use of GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/gpu:0']\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "sess= K.get_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Humanoid-v2')\n",
    "So = env.reset()\n",
    "A = env.action_space.sample()\n",
    "number_of_neurons= 64\n",
    "layer_dropout = 0.5\n",
    "input_dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting learning hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Policy\n",
    "policy_alpha = 0.01\n",
    "policy_lambda =  0\n",
    "#Value\n",
    "value_alpha = 0.01\n",
    "value_lambda = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model for policy mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Mean network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_1 (Dropout)          (None, 376)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                24128     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 17)                1105      \n",
      "=================================================================\n",
      "Total params: 33,553\n",
      "Trainable params: 33,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "policy_model_mean = Sequential()\n",
    "policy_model_mean.add(Dropout(input_dropout,input_shape=So.shape))\n",
    "policy_model_mean.add(Dense(number_of_neurons ,kernel_initializer='random_uniform', activation = 'relu' ))\n",
    "policy_model_mean.add(Dropout(layer_dropout))\n",
    "policy_model_mean.add(Dense(number_of_neurons ,kernel_initializer='random_uniform', activation = 'relu' ))\n",
    "policy_model_mean.add(Dropout(layer_dropout))\n",
    "policy_model_mean.add(Dense(number_of_neurons,kernel_initializer='random_uniform', activation = 'relu'))\n",
    "policy_model_mean.add(Dropout(layer_dropout))\n",
    "policy_model_mean.add(Dense( A.shape[0],kernel_initializer='random_uniform', activation='linear' ))\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "policy_model_mean.compile(loss=\"mse\", optimizer= adam)\n",
    "print(\"Policy Mean network\")\n",
    "print(policy_model_mean.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model for policy std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy std network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_5 (Dropout)          (None, 376)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                24128     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 17)                1105      \n",
      "=================================================================\n",
      "Total params: 33,553\n",
      "Trainable params: 33,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "policy_model_std = Sequential()\n",
    "policy_model_std.add(Dropout(input_dropout,input_shape=So.shape))\n",
    "policy_model_std.add(Dense(number_of_neurons ,kernel_initializer='random_uniform', activation = 'relu' ))\n",
    "policy_model_std.add(Dropout(layer_dropout))\n",
    "policy_model_std.add(Dense(number_of_neurons ,kernel_initializer='random_uniform', activation = 'relu' ))\n",
    "policy_model_std.add(Dropout(layer_dropout))\n",
    "policy_model_std.add(Dense(number_of_neurons,kernel_initializer='random_uniform', activation = 'relu'))\n",
    "policy_model_std.add(Dropout(layer_dropout))\n",
    "policy_model_std.add(Dense( A.shape[0],kernel_initializer='random_uniform', activation='linear' ))\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "policy_model_std.compile(loss=\"mse\", optimizer= adam)\n",
    "print(\"Policy std network\")\n",
    "print(policy_model_std.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating model for values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_9 (Dropout)          (None, 376)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                24128     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 32,513\n",
      "Trainable params: 32,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "value_model = Sequential()\n",
    "value_model.add(Dropout(input_dropout,input_shape=So.shape))\n",
    "value_model.add(Dense(number_of_neurons, kernel_initializer='random_uniform',activation = 'relu' ))\n",
    "value_model.add(Dropout(layer_dropout))\n",
    "value_model.add(Dense(number_of_neurons, kernel_initializer='random_uniform',activation = 'relu'))\n",
    "value_model.add(Dropout(layer_dropout))\n",
    "value_model.add(Dense(number_of_neurons, kernel_initializer='random_uniform',activation = 'relu'))\n",
    "value_model.add(Dropout(layer_dropout))\n",
    "value_model.add(Dense(1,kernel_initializer='random_uniform', activation= 'linear'))\n",
    "\n",
    "print(\"Value Model\")\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "value_model.compile(loss=\"mse\", optimizer= adam)\n",
    "print(value_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eligibility Traces for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "with tf.name_scope(\"eligibility_traces\"):\n",
    "    with tf.name_scope(\"MEAN\"):\n",
    "        policy_eligibility_traces_mean= [tf.Variable(np.zeros(shape = sess.run(tf.shape(tensor))), dtype= tf.float32) for tensor in policy_model_mean.trainable_weights]\n",
    "        variable_summaries(policy_eligibility_traces_mean)\n",
    "    with tf.name_scope(\"STD\"):\n",
    "        policy_eligibility_traces_std= [tf.Variable(np.zeros(shape = sess.run(tf.shape(tensor))), dtype= tf.float32) for tensor in policy_model_std.trainable_weights]\n",
    "        variable_summaries(policy_eligibility_traces_std)\n",
    "    with tf.name_scope(\"VALUE\"):\n",
    "        value_eligibility_traces= [tf.Variable(np.zeros(shape = sess.run(tf.shape(tensor))), dtype= tf.float32) for tensor in value_model.trainable_weights]\n",
    "        variable_summaries(value_eligibility_traces)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining other hyperparameters concerning the reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_reward = 5 #pickle.load(open('average_reward.ferch','rb'))\n",
    "cumulative_reward=0\n",
    "etaa = 0.0001\n",
    "clip_norm = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(False):\n",
    "    policy_model_mean = load_model('policy_mean.h5')\n",
    "    policy_model_std = load_model('policy_std.h5')\n",
    "    value_model = load_model('value.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"weights\"):\n",
    "    \n",
    "    with tf.name_scope(\"VALUE\"):\n",
    "        value_weights = value_model.trainable_weights\n",
    "        variable_summaries(value_weights)\n",
    "    with tf.name_scope(\"MEAN\"):\n",
    "        policy_weights_mean = policy_model_mean.trainable_weights\n",
    "        variable_summaries(policy_weights_mean)\n",
    "    with tf.name_scope(\"STD\"):\n",
    "        policy_weights_std = policy_model_std.trainable_weights\n",
    "        variable_summaries(policy_weights_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Rewards\"):\n",
    "    with tf.name_scope(\"R\"):\n",
    "        R = tf.Variable(0, dtype= tf.float32)\n",
    "        variable_summaries2(R)\n",
    "    with tf.name_scope(\"Delta_R\"):\n",
    "        Delta_R = tf.Variable(0, dtype= tf.float32)\n",
    "        variable_summaries2(Delta_R)\n",
    "    with tf.name_scope(\"Average_R\"):\n",
    "        Average_R = tf.Variable(0, dtype= tf.float32)\n",
    "        variable_summaries2(Average_R)\n",
    "    \n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALUE OPS\n",
    "global value_eligibility_traces , value_weights , policy_weights_std\n",
    "value_gradients = value_model.optimizer.get_gradients(value_model.output, value_weights)\n",
    "Delta_rewards = tf.placeholder(tf.float32)\n",
    "value_gradients = [tf.clip_by_norm(gradient, clip_norm) for gradient in value_gradients]\n",
    "value_eligibility_traces_op = [value_eligibility_traces[i].assign(tf.add(tf.multiply(value_eligibility_traces[i], tf.constant(value_lambda, dtype = tf.float32)),value_gradients[i] )) for i in range(len(value_eligibility_traces))] \n",
    "value_weights_op =  [value_weights[i].assign(tf.add(value_weights[i], tf.multiply(value_alpha*Delta_rewards, value_eligibility_traces[i]))) for i in range(len(value_weights))]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEAN OPS\n",
    "global policy_eligibility_traces_mean, policy_eligibility_traces_std , policy_weights_mean\n",
    "Action = tf.placeholder(tf.float32, shape = A.shape)\n",
    "Policy_std = tf.placeholder(tf.float32)\n",
    "loss_mean = tf.divide(tf.subtract(Action, policy_model_mean.output), Policy_std*Policy_std)\n",
    "policy_gradients_mean = policy_model_mean.optimizer.get_gradients(loss_mean, policy_weights_mean)\n",
    "policy_gradients_mean = [tf.clip_by_norm(gradient, clip_norm) for gradient in policy_gradients_mean]\n",
    "policy_eligibility_traces_op_mean = [policy_eligibility_traces_mean[i].assign(tf.add(tf.multiply(tf.constant(policy_lambda, dtype = tf.float32), policy_eligibility_traces_mean[i]), policy_gradients_mean[i])) for i in range(len(policy_gradients_mean))]\n",
    "\n",
    "policy_weights_op_mean =  [policy_weights_mean[i].assign(tf.add(policy_weights_mean[i], tf.multiply(policy_eligibility_traces_mean[i],policy_alpha*Delta_rewards))) for i in range(len(policy_weights_mean))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#STD OPS\n",
    "global policy_eligibility_traces_mean, policy_eligibility_traces_std , policy_weights_mean\n",
    "Policy_mean =tf.placeholder(tf.float32)\n",
    "loss_std = 2* tf.subtract(tf.divide(tf.square(tf.subtract(Action,Policy_mean)), tf.pow(policy_model_std.output, 3)), tf.divide(1, policy_model_std.output))\n",
    "policy_gradients_std = policy_model_std.optimizer.get_gradients(loss_std,policy_weights_std)\n",
    "policy_gradients_std = [tf.clip_by_norm(gradient, clip_norm) for gradient in policy_gradients_std]\n",
    "policy_eligibility_traces_op_std = [policy_eligibility_traces_std[i].assign(tf.add(tf.multiply(tf.constant(policy_lambda, dtype = tf.float32), policy_eligibility_traces_std[i]), policy_gradients_std[i])) for i in range(len(policy_gradients_std))]\n",
    "\n",
    "policy_weights_op_std =  [policy_weights_std[i].assign(tf.add(policy_weights_std[i], tf.multiply( policy_eligibility_traces_std[i],policy_alpha*Delta_rewards))) for i in range(len(policy_weights_std))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(previous_state, action , reward, state,  terminal, skip_count):\n",
    "    \n",
    "    #Compute rewards\n",
    "    global average_reward,rewardz_received,delta_rewardz_received, cumulative_reward \n",
    "    delta_rewards = reward - average_reward*skip_count + value_model.predict(np.array([state]))[0][0] - value_model.predict(np.array([previous_state]))[0][0]    \n",
    "    cumulative_reward = (cumulative_reward + reward)/2\n",
    "    print(value_model.predict(np.array([previous_state]))[0][0] ,\"=====\",value_model.predict(np.array([state]))[0][0],\"====\",delta_rewards)\n",
    "    if(terminal):\n",
    "        delta_rewards = reward - average_reward*skip_count + 0  - value_model.predict(np.array([previous_state]))[0][0]\n",
    "    thresh = average_reward + cumulative_reward\n",
    "    if(delta_rewards>=thresh):\n",
    "        delta_rewards =thresh\n",
    "    if(delta_rewards<=-thresh):\n",
    "        delta_rewards=-thresh\n",
    "    average_reward = average_reward + etaa*delta_rewards\n",
    "    sess.run(Delta_R.assign(delta_rewards))\n",
    "    sess.run(Average_R.assign(average_reward))\n",
    "    \n",
    "    #Compute value updates (eligibility traces and weights)\n",
    "    sess.run(value_eligibility_traces_op,feed_dict={value_model.input:np.array([state])})    \n",
    "    sess.run(value_weights_op, feed_dict={Delta_rewards:delta_rewards})\n",
    "    \n",
    "    #Compute policy updates (eligibility traces and weights)\n",
    "\n",
    "    policy_mean = policy_model_mean.predict(np.array([previous_state]))[0]\n",
    "    policy_std = policy_model_std.predict(np.array([previous_state]))[0]\n",
    "    policy_std= policy_std*policy_std\n",
    "    \n",
    "    ##################################################  MEAN ##################################################################\n",
    "    sess.run(policy_eligibility_traces_op_mean, feed_dict={policy_model_mean.input:np.array([state]), Action:action, Policy_std:policy_std})\n",
    "    sess.run(policy_weights_op_mean, feed_dict={Delta_rewards:delta_rewards})    \n",
    "    \n",
    "    ##################################################   STD ###################################################################\n",
    "    sess.run(policy_eligibility_traces_op_std, feed_dict={policy_model_std.input:np.array([state]), Action:action, Policy_mean:policy_mean})\n",
    "    sess.run(policy_weights_op_std, feed_dict={Delta_rewards:delta_rewards} )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    \n",
    "    mean = policy_model_mean.predict(np.array([state]))[0]\n",
    "    std = policy_model_std.predict(np.array([state]))[0]\n",
    "    std= std*std\n",
    "    #print(mean)\n",
    "    covariance = np.zeros([mean.shape[0],mean.shape[0]])\n",
    "    \n",
    "    for i in range(std.shape[0]):\n",
    "        covariance[i][i]= std[i]\n",
    "        \n",
    "    action = np.random.multivariate_normal(mean, covariance)\n",
    "    #print(action)\n",
    "\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'      \\n    #FIXING ACTION INCONSISTENCIES\\n    low = env.action_space.low\\n    high = env.action_space.high\\n    \\n    for i in range(action.shape[0]):\\n        if(action[i]>high[i]):\\n            action[i]= high[i]+0.1\\n        if(action[i]<low[i]):\\n            action[i]=low[i]+0.1\\n            \\\\    high = env.action_space.high\\n    global average_reward_variance\\n    print(average_reward_variance)\\n    if(np.abs(average_reward_variance)<0.1):\\n        print(\"Sampling randomly\")\\n        return env.action_space.sample()\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''      \n",
    "    #FIXING ACTION INCONSISTENCIES\n",
    "    low = env.action_space.low\n",
    "    high = env.action_space.high\n",
    "    \n",
    "    for i in range(action.shape[0]):\n",
    "        if(action[i]>high[i]):\n",
    "            action[i]= high[i]+0.1\n",
    "        if(action[i]<low[i]):\n",
    "            action[i]=low[i]+0.1\n",
    "            \\    high = env.action_space.high\n",
    "    global average_reward_variance\n",
    "    print(average_reward_variance)\n",
    "    if(np.abs(average_reward_variance)<0.1):\n",
    "        print(\"Sampling randomly\")\n",
    "        return env.action_space.sample()\n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Episodes\n",
    "## Setting up variables :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S = env.reset()\n",
    "#action_count=0\n",
    "episode_count=0\n",
    "save= True\n",
    "total_reward =0 \n",
    "total_reward_list=[]\n",
    "skip_count = 0\n",
    "skip_reward=0\n",
    "action_count= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./logz/\",sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy_eligibility_traces_mean_reset= [policy_eligibility_traces_mean[i].assign(np.zeros(shape = sess.run(tf.shape(policy_eligibility_traces_mean[i])))) for i in range(len(policy_eligibility_traces_mean))]\n",
    "policy_eligibility_traces_std_reset=  [policy_eligibility_traces_std[i].assign(np.zeros(shape = sess.run(tf.shape(policy_eligibility_traces_std[i])))) for i in range(len(policy_eligibility_traces_std))]\n",
    "value_eligibility_traces_reset=  [value_eligibility_traces[i].assign(np.zeros(shape = sess.run(tf.shape(value_eligibility_traces[i])))) for i in range(len(value_eligibility_traces))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000511149 ===== 0.000504193 ==== -0.000334515088331\n",
      "0.000500695 ===== 0.000513432 ==== -0.000314132384291\n",
      "0.000510146 ===== 0.00062529 ==== -0.00021117556882\n",
      "0.00062308 ===== 0.000648572 ==== -0.000301812302636\n",
      "0.000645432 ===== 0.000714968 ==== -0.000257717175486\n",
      "0.000712271 ===== 0.000742662 ==== -0.000297177064658\n",
      "0.000739553 ===== 0.00082582 ==== -0.000240192371992\n",
      "0.00082331 ===== 0.000808136 ==== -0.000341736411198\n",
      "0.00080457 ===== 0.000916214 ==== -0.000215034655642\n",
      "0.000913971 ===== 0.000939238 ==== -0.000301263152289\n",
      "0.000936076 ===== 0.242206 ==== -1.1345637387\n",
      "-0.0800078 ===== -0.0391091 ==== -0.157661149425\n",
      "-0.0610922 ===== -0.0343816 ==== 0.0226755556953\n",
      "-0.032843 ===== -0.0252555 ==== 0.0316362230676\n",
      "-0.0239016 ===== -0.0242112 ==== 0.0246760135365\n",
      "-0.023213 ===== -0.0251616 ==== 0.0205092774106\n",
      "-0.0242209 ===== -0.0263062 ==== 0.0196812094075\n",
      "-0.025298 ===== -0.0257095 ==== 0.0243215027871\n",
      "-0.024381 ===== -0.025938 ==== 0.0296161653008\n",
      "-0.0243169 ===== -0.0240848 ==== 0.0401585299706\n",
      "-0.0218754 ===== -0.0224415 ==== 0.0504662050275\n",
      "-0.0196896 ===== -0.0189638 ==== 0.0642522964483\n",
      "-0.015498 ===== -0.0159034 ==== 0.0770548010899\n",
      "-0.0118203 ===== -0.0128199 ==== 0.0921565208165\n",
      "-0.00811195 ===== -0.0070166 ==== 0.110635361293\n",
      "-0.00159776 ===== -0.00287721 ==== 0.125424348421\n",
      "0.00283504 ===== 0.00127669 ==== 0.14235889984\n",
      "0.00742979 ===== 0.00761823 ==== 0.162276084656\n",
      "0.0137174 ===== 0.0129009 ==== 0.181020069938\n",
      "0.0191454 ===== 0.0176907 ==== 0.200208921005\n",
      "0.0245563 ===== 0.0237303 ==== 0.216099375021\n",
      "0.0318065 ===== 0.0333112 ==== 0.238574161369\n",
      "0.0418227 ===== 0.0402412 ==== 0.253386475086\n",
      "0.0492704 ===== 0.0451074 ==== 0.269583816654\n",
      "0.0544124 ===== 0.0467627 ==== 0.281269005889\n",
      "0.0582745 ===== 0.0541309 ==== 0.303255504905\n",
      "0.0635822 ===== 0.0680739 ==== 0.333660686126\n",
      "0.0782356 ===== 0.0825503 ==== 0.351704424027\n",
      "0.0932426 ===== 0.0911827 ==== 0.361629189748\n",
      "0.10212 ===== 0.0493249 ==== 0.331762108116\n",
      "0.055616 ===== 0.0635299 ==== 0.407421140577\n",
      "0 --total reward =208.221522517\n",
      "0 --average reward =5.00034089043\n",
      "0.0376341 ===== 0.0383128 ==== 0.00436039404242\n",
      "0.0383588 ===== 0.037688 ==== 0.00307228548324\n",
      "0.0377205 ===== 0.0362048 ==== 0.00209053823138\n",
      "0.0362273 ===== 0.0380633 ==== 0.00549311351278\n",
      "0.0381221 ===== 0.0383569 ==== 0.0039526428893\n",
      "0.0383987 ===== 0.0398884 ==== 0.00507607461118\n",
      "0.0399425 ===== 0.0387375 ==== 0.00237376756347\n",
      "0.0387627 ===== 0.0385867 ==== 0.00348666159031\n",
      "0.0386233 ===== 0.0388231 ==== 0.003917033873\n",
      "0.0388647 ===== 1.20342 ==== -1.3038820981\n",
      "0.611267 ===== 0.266605 ==== -0.664915223437\n",
      "0.154985 ===== 0.0715769 ==== -0.0178465647478\n",
      "0.0703253 ===== 0.0412321 ==== 0.0968941533374\n",
      "0.0447121 ===== 0.038887 ==== 0.130104698759\n",
      "0.0424564 ===== 0.0440374 ==== 0.144294119338\n",
      "0.0481434 ===== 0.0547409 ==== 0.155413073943\n",
      "0.0607085 ===== 0.0624103 ==== 0.160933928618\n",
      "0.0685672 ===== 0.0705772 ==== 0.173421472713\n",
      "0.0775151 ===== 0.0769883 ==== 0.18663964816\n",
      "0.0846562 ===== 0.0779787 ==== 0.202993932257\n",
      "0.0851462 ===== 0.0521823 ==== 0.20379998046\n",
      "0.0574507 ===== 0.0958687 ==== 0.287224371594\n",
      "0.107137 ===== 0.112596 ==== 0.271506091349\n",
      "0.124368 ===== 0.112523 ==== 0.275506006889\n",
      "0.123525 ===== 0.121921 ==== 0.3048483396\n",
      "0.133769 ===== 0.124095 ==== 0.31697623634\n",
      "0.135107 ===== 0.131935 ==== 0.342610640778\n",
      "0.142982 ===== 0.141194 ==== 0.361332064051\n",
      "0.153001 ===== 0.138243 ==== 0.365572034926\n",
      "0.148873 ===== 0.100677 ==== 0.353206105412\n",
      "0.109197 ===== 0.15933 ==== 0.465413547691\n",
      "0.174849 ===== 0.172481 ==== 0.428649407776\n",
      "0.187147 ===== 0.159426 ==== 0.419565846301\n",
      "1 --total reward =168.058217186\n",
      "1 --average reward =5.00069435627\n",
      "0.0771854 ===== 0.0767924 ==== -0.010508977284\n",
      "0.076676 ===== 0.0781973 ==== -0.0083382352243\n",
      "0.0781067 ===== 0.0768852 ==== -0.0112268493029\n",
      "0.0767634 ===== 0.0775218 ==== -0.00927403105253\n",
      "0.077421 ===== 0.0780863 ==== -0.00962957956414\n",
      "0.0779799 ===== 0.0754753 ==== -0.013097868379\n",
      "0.0753305 ===== 0.0753693 ==== -0.0104802861518\n",
      "0.0752543 ===== 0.0764449 ==== -0.00911401790202\n",
      "0.0763455 ===== 0.0777181 ==== -0.00872073059278\n",
      "0.0776229 ===== 0.0750009 ==== -0.0129302196141\n",
      "0.0748582 ===== 2.19474 ==== -0.299576377682\n",
      "2.01531 ===== 0.787946 ==== -1.41350799255\n",
      "0.488983 ===== 0.211126 ==== -0.102952805987\n",
      "0.202545 ===== 0.133978 ==== 0.149719959191\n",
      "0.140349 ===== 0.131165 ==== 0.210954968377\n",
      "0.138642 ===== 0.136914 ==== 0.222608982732\n",
      "0.144269 ===== 0.140696 ==== 0.230226939578\n",
      "0.148088 ===== 0.198551 ==== 0.283353512662\n",
      "0.215346 ===== 0.195328 ==== 0.238876520754\n",
      "0.207413 ===== 0.189187 ==== 0.265612322391\n",
      "0.200912 ===== 0.205745 ==== 0.30599071064\n",
      "0.220315 ===== 0.22421 ==== 0.322415799032\n",
      "0.240718 ===== 0.145113 ==== 0.246448972162\n",
      "0.15235 ===== 0.21534 ==== 0.418680580255\n",
      "0.234667 ===== 0.246145 ==== 0.383654974248\n",
      "0.265451 ===== 0.159577 ==== 0.290094230633\n",
      "0.167108 ===== 0.196988 ==== 0.437975093995\n",
      "0.215074 ===== 0.198064 ==== 0.405452100258\n",
      "0.212018 ===== 0.272927 ==== 0.494960563118\n",
      "0.297861 ===== 0.284335 ==== 0.433153972297\n",
      "2 --total reward =152.95707487\n",
      "2 --average reward =5.00100800504\n",
      "0.114734 ===== 0.116951 ==== -0.006310934259\n",
      "0.116878 ===== 0.115217 ==== -0.0109170554983\n",
      "0.115092 ===== 0.112462 ==== -0.0116645063813\n",
      "0.112329 ===== 0.115586 ==== -0.00583694925624\n",
      "0.115519 ===== 0.118563 ==== -0.00551035490583\n",
      "0.1185 ===== 0.11544 ==== -0.0118014724018\n",
      "0.115305 ===== 0.120146 ==== -0.00443081356601\n",
      "0.120093 ===== 0.113277 ==== -0.015934556085\n",
      "0.113093 ===== 0.110713 ==== -0.0114129071386\n",
      "0.110584 ===== 0.113177 ==== -0.00620780538396\n",
      "0.113105 ===== 3.64137 ==== 1.34024805416\n",
      "4.78617 ===== 1.90375 ==== -3.10796801799\n",
      "0.973946 ===== 0.425912 ==== -0.456804579471\n",
      "0.373968 ===== 0.222449 ==== -0.00314631716396\n",
      "0.222251 ===== 0.172641 ==== 0.121537988915\n",
      "0.177599 ===== 0.175141 ==== 0.182814152028\n",
      "0.180288 ===== 0.0915609 ==== 0.110261544203\n",
      "0.0928458 ===== 0.301776 ==== 0.393977482891\n",
      "0.328779 ===== 0.263662 ==== 0.154019780535\n",
      "0.27196 ===== 0.169181 ==== 0.141660558201\n",
      "0.173616 ===== 0.30648 ==== 0.382104840509\n",
      "0.330678 ===== 0.243095 ==== 0.18093487752\n",
      "0.253085 ===== 0.205715 ==== 0.241136303316\n",
      "0.214389 ===== 0.242323 ==== 0.332626255127\n",
      "0.259106 ===== 0.179848 ==== 0.245316083927\n",
      "0.186358 ===== 0.189003 ==== 0.341888061538\n",
      "0.198721 ===== 0.124661 ==== 0.282460579661\n",
      "0.128119 ===== 0.129933 ==== 0.375110998299\n",
      "0.134606 ===== 0.222428 ==== 0.4715088599\n",
      "0.236663 ===== 0.297291 ==== 0.452719800246\n",
      "3 --total reward =152.249882166\n",
      "3 --average reward =5.00118751395\n",
      "0.135251 ===== 0.138535 ==== 0.00479423438988\n",
      "0.138592 ===== 0.136592 ==== -0.000756334570307\n",
      "0.136583 ===== 0.137814 ==== 0.00317360921779\n",
      "0.137851 ===== 0.1372 ==== 0.000936764178161\n",
      "0.137211 ===== 0.139822 ==== 0.0043279818789\n",
      "0.139875 ===== 0.139631 ==== 0.0014111825259\n",
      "0.139648 ===== 0.142531 ==== 0.00427914293418\n",
      "0.142582 ===== 0.140522 ==== -9.46073743862e-05\n",
      "0.140521 ===== 0.138635 ==== -0.0005520066817\n",
      "0.138628 ===== 0.141017 ==== 0.00415090572931\n",
      "0.141067 ===== 1.87285 ==== 0.965973262873\n",
      "2.35672 ===== 1.81233 ==== -0.692068535569\n",
      "1.57252 ===== 0.616569 ==== -0.70864710175\n",
      "0.534544 ===== 0.282289 ==== 0.05679757862\n",
      "0.284973 ===== 0.157984 ==== 0.192188885144\n",
      "0.160811 ===== 0.440444 ==== 0.577506923908\n",
      "0.492132 ===== 0.309239 ==== 0.143360603764\n",
      "0.316433 ===== 0.231352 ==== 0.252135212223\n",
      "0.241317 ===== 0.332672 ==== 0.439974123662\n",
      "0.357607 ===== 0.367723 ==== 0.373667790084\n",
      "0.390662 ===== 0.226227 ==== 0.215217847792\n",
      "0.23196 ===== 0.348494 ==== 0.504497378238\n",
      "0.376171 ===== 0.316228 ==== 0.335625820952\n",
      "0.33067 ===== 0.405687 ==== 0.47711280586\n",
      "4 --total reward =123.246649953\n",
      "4 --average reward =5.00146244662\n",
      "0.171084 ===== 0.178285 ==== -0.00325338212225\n",
      "0.178241 ===== 0.17211 ==== -0.0162634968888\n",
      "0.1719 ===== 0.175776 ==== -0.00619057666182\n",
      "0.175694 ===== 0.171683 ==== -0.0145788185868\n",
      "0.171491 ===== 0.173675 ==== -0.00782238750093\n",
      "0.173572 ===== 0.176741 ==== -0.00698039871168\n",
      "0.176646 ===== 0.174321 ==== -0.0127431077259\n",
      "0.174149 ===== 0.173968 ==== -0.0106132772079\n",
      "0.173827 ===== 0.171755 ==== -0.0121213431752\n",
      "0.171596 ===== 0.178163 ==== -0.00405204860505\n",
      "0.178107 ===== 3.56629 ==== 2.33968277216\n",
      "5.45642 ===== 2.54144 ==== -2.98772662165\n",
      "1.48749 ===== 0.758444 ==== -0.579551758525\n",
      "0.681466 ===== 0.729851 ==== 0.19733382976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76251 ===== 0.369479 ==== -0.121317499824\n",
      "0.360556 ===== 0.20946 ==== 0.161165800153\n",
      "0.212947 ===== 0.494326 ==== 0.597174187145\n",
      "0.546079 ===== 0.371697 ==== 0.165098477242\n",
      "0.381175 ===== 0.476945 ==== 0.438380873721\n",
      "0.511533 ===== 0.447588 ==== 0.296504236657\n",
      "0.468208 ===== 0.457618 ==== 0.360006353838\n",
      "0.482701 ===== 0.583585 ==== 0.484543267324\n",
      "0.630465 ===== 0.532041 ==== 0.314504758501\n",
      "5 --total reward =117.217232152\n",
      "5 --average reward =5.00156636049\n",
      "0.185476 ===== 0.194675 ==== -0.00766936411243\n",
      "0.194565 ===== 0.197825 ==== -0.0136325185674\n",
      "0.19763 ===== 0.195536 ==== -0.0188508187724\n",
      "0.195267 ===== 0.196175 ==== -0.0162095411549\n",
      "0.195906 ===== 0.193069 ==== -0.0199471134964\n",
      "0.192781 ===== 0.197591 ==== -0.0118268259885\n",
      "0.197422 ===== 0.199826 ==== -0.0142521925807\n",
      "0.199622 ===== 0.194405 ==== -0.0222033109088\n",
      "0.194069 ===== 0.196349 ==== -0.0148275137278\n",
      "0.196109 ===== 4.46573 ==== 3.05365337366\n",
      "7.52008 ===== 3.69897 ==== -3.9800785925\n",
      "1.8794 ===== 0.731328 ==== -1.00816926911\n",
      "0.612457 ===== 0.374317 ==== -0.0620187343591\n",
      "0.370609 ===== 0.169558 ==== -0.0165223582489\n",
      "0.16934 ===== 0.168785 ==== 0.183056902368\n",
      "0.17131 ===== 0.47237 ==== 0.479932200687\n",
      "0.511422 ===== 0.387582 ==== 0.076629972867\n",
      "0.392123 ===== 0.441036 ==== 0.253522347553\n",
      "0.45872 ===== 0.456853 ==== 0.210882036745\n",
      "0.47223 ===== 0.18404 ==== -0.0557247454048\n",
      "0.183204 ===== 0.184994 ==== 0.247749177822\n",
      "0.188491 ===== 0.441673 ==== 0.494186800957\n",
      "6 --total reward =110.706296593\n",
      "6 --average reward =5.00149596117\n",
      "0.174554 ===== 0.181197 ==== -0.00035975248569\n",
      "0.181192 ===== 0.185361 ==== -0.00365260739784\n",
      "0.185311 ===== 0.187532 ==== -0.00481035759051\n",
      "0.187467 ===== 0.185912 ==== -0.00884721250321\n",
      "0.185792 ===== 0.194605 ==== 0.000117209484013\n",
      "0.194607 ===== 0.190708 ==== -0.0127713828467\n",
      "0.19053 ===== 0.189044 ==== -0.00872125130991\n",
      "0.188924 ===== 0.18345 ==== -0.0125267833514\n",
      "0.183269 ===== 0.190863 ==== 1.68508909484e-05\n",
      "0.190864 ===== 3.73176 ==== 2.33256621318\n",
      "5.70291 ===== 3.21883 ==== -2.74878585127\n",
      "1.96243 ===== 1.18897 ==== -0.736428510554\n",
      "1.03273 ===== 0.411706 ==== -0.445255233156\n",
      "0.382417 ===== 0.166503 ==== -0.0252199174058\n",
      "0.166165 ===== 0.160166 ==== 0.186599506411\n",
      "0.162576 ===== 0.161574 ==== 0.191575839333\n",
      "0.164312 ===== 0.414484 ==== 0.442997801524\n",
      "0.447331 ===== 0.287285 ==== 0.0574306023661\n",
      "0.289352 ===== 0.171787 ==== 0.107839923724\n",
      "0.173216 ===== 0.319429 ==== 0.377559869231\n",
      "0.336482 ===== 0.363781 ==== 0.266812080572\n",
      "0.379014 ===== 0.383719 ==== 0.255518488192\n",
      "0.398688 ===== 0.188764 ==== 0.0552094449629\n",
      "7 --total reward =115.903140662\n",
      "7 --average reward =5.00150377129\n",
      "0.174594 ===== 0.194482 ==== 0.0120318436058\n",
      "0.194657 ===== 0.19094 ==== -0.0116715210637\n",
      "0.190758 ===== 0.181615 ==== -0.0150301836465\n",
      "0.181413 ===== 0.18672 ==== -0.000190802473229\n",
      "0.186717 ===== 0.185638 ==== -0.00682994170676\n",
      "0.185539 ===== 0.184349 ==== -0.00881007375905\n",
      "0.184222 ===== 0.189402 ==== -0.00162396797665\n",
      "0.189379 ===== 0.189698 ==== -0.0069008284226\n",
      "0.189589 ===== 0.186822 ==== -0.00979037319111\n",
      "0.186682 ===== 0.187734 ==== -0.00671225341006\n",
      "0.187636 ===== 0.189476 ==== -0.00491719864586\n",
      "0.189402 ===== 1.70052 ==== 1.32262007407\n",
      "2.17244 ===== 0.856906 ==== -1.15604256056\n",
      "0.706545 ===== 0.369122 ==== -0.114798007854\n",
      "0.362734 ===== 0.189534 ==== 0.0689272145044\n",
      "0.190573 ===== 0.193861 ==== 0.244638551431\n",
      "0.197787 ===== 0.565038 ==== 0.593817428855\n",
      "0.622342 ===== 0.364836 ==== 0.0050864877449\n",
      "0.365082 ===== 0.210477 ==== 0.12349563053\n",
      "8 --total reward =96.3971098328\n",
      "8 --average reward =5.00158545358\n",
      "0.185291 ===== 0.193131 ==== 0.00085230831892\n",
      "0.193144 ===== 0.19509 ==== -0.00489223280007\n",
      "0.195019 ===== 0.201814 ==== -0.00104282048683\n",
      "0.201798 ===== 0.20902 ==== -0.000124310075019\n",
      "0.209018 ===== 0.202978 ==== -0.0125746697037\n",
      "0.202783 ===== 0.203227 ==== -0.00561324072912\n",
      "0.203139 ===== 0.206865 ==== -0.0032707780548\n",
      "0.206811 ===== 0.207151 ==== -0.00672130366707\n",
      "0.207038 ===== 0.205191 ==== -0.0125484998942\n",
      "0.20498 ===== 0.209713 ==== -0.00231563491046\n",
      "0.209674 ===== 0.201266 ==== -0.015795710131\n",
      "0.20101 ===== 0.199132 ==== -0.0104992702051\n",
      "0.19896 ===== 0.209493 ==== 0.00365659993931\n",
      "0.209562 ===== 1.7087 ==== 1.30160146653\n",
      "2.1617 ===== 0.757395 ==== -1.26936277895\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    \n",
    "    #renders environment \n",
    "    #env.render()\n",
    "    \n",
    "    high = env.action_space.high\n",
    "    #Selects action according to stochastic policy\n",
    "    action = select_action(S%high[0])\n",
    "    if(action_count%50==0):\n",
    "        summary = sess.run(merged)\n",
    "        writer.add_summary(summary, action_count)\n",
    "    action_count+=1\n",
    "    \n",
    "    \n",
    "    #Takes action \n",
    "    S1, reward, done, info = env.step(action)\n",
    "    skip_reward += reward\n",
    "    skip_count += 1\n",
    "    sess.run(R.assign(reward))\n",
    "    \n",
    "    #Updates weights\n",
    "    #if(np.random.randint(10)%5==0):\n",
    "    if(True):\n",
    "        update_weights(S , action, skip_reward, S1 , done, skip_count)\n",
    "        skip_reward = 0\n",
    "        skip_count =0\n",
    "    \n",
    "    S = S1\n",
    "    total_reward +=reward\n",
    "    \n",
    "    if(done):\n",
    "        \n",
    "        #print('We are now at '+str(episode_count))\n",
    "        gc.collect()\n",
    "        print(str(episode_count)+' --total reward ='+ str(total_reward))\n",
    "        print(str(episode_count)+' --average reward ='+ str(average_reward))\n",
    "        total_reward_list.append(total_reward)\n",
    "        total_reward=0\n",
    "        \n",
    "        #Resets episode\n",
    "        S = env.reset()\n",
    "        #action_count=0\n",
    "        episode_count+=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(episode_count%10==0):\n",
    "            \n",
    "            if(save):\n",
    "                print('saving models')\n",
    "                print('average reward '+ str(average_reward))\n",
    "                \n",
    "                policy_model_mean.save('policy_mean.h5')\n",
    "                policy_model_std.save('policy_std.h5')\n",
    "                value_model.save('value.h5')\n",
    "                pickle.dump(total_reward_list,open('totalz_hist','wb')) \n",
    "                \n",
    "                \n",
    "                #Restarting keras session \n",
    "                '''\n",
    "                K.clear_session()\n",
    "                sess = K.get_session()\n",
    "                policy_model_mean = load_model('policy_mean.h5')\n",
    "                policy_model_std = load_model('policy_std.h5')\n",
    "                value_model = load_model('value.h5')\n",
    "                '''\n",
    "\n",
    "        \n",
    "        #Resets eligibility traces\n",
    "        sess.run([policy_eligibility_traces_mean_reset,policy_eligibility_traces_std_reset,value_eligibility_traces_reset])\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
